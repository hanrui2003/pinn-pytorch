\documentclass{article}
\usepackage[a4paper,innermargin=1.2in,outermargin=1.2in,
bottom=1.5in,marginparwidth=1in,marginparsep=3mm]{geometry}
\usepackage{amsmath,amsthm,amssymb,enumerate}
\usepackage{ctex}
\usepackage{natbib}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{float}
\usepackage{subfigure}
\usepackage{color}
\usepackage{array}
\usepackage{multirow}
\title{基于神经网络的数据同化研究}
\author{MG21210021李庆春}
%\date{2022.8.20}
\linespread{1.25}
\bibliographystyle{plain}



\begin{document}
\maketitle
\section{绪论}
文献\cite{st-rfm}提出了一种新型的微分方程数值解法，RFM的全称是The Random Feature Method，意为随机特征方法。目的是解决经典数值方法和机器学习方法的痛点，即经典的微分方程数值方法通常具有稳定的收敛阶，但依赖于网格离散的特性让它们难以处理复杂几何上的问题；而机器学习方法在这类复杂问题上具有优势，但求解误差不可控，且相比于经典方法需要很长的求解时间。作为一种新型的微分方程数值方法，融合了经典方法和机器学习方法的优势，是一种兼具稳定收敛性和简便性的新型微分方程数值方法。

科学计算中最古老且研究最广泛的主题之一是用于解决偏微分方程（PDEs）的算法。已经提出并广泛研究了有限差分[13]、有限元[25]、谱方法[20]以及许多其他方法，并取得了巨大成功。与此同时，基于这些方法的各种科学软件已经被开发出来，并广泛应用于学术界和工业界。它们已经成为几乎所有工程应用中的标准资源。

近年来，随着神经网络模型在各种人工智能（AI）任务中取得了巨大成功，将这些模型用于解决PDEs的想法变得非常流行[5, 7, 9, 18, 22, 24]。虽然早在90年代，就已经提出了在PDE求解器中使用神经网络作为测试或试验函数的想法[12]，但最近的提议通常具有一些非常重要的新变化。最显著的成功是解决高维度的PDEs和控制问题[5, 8, 9, 22]，这是传统算法无法处理的问题类别。事实上，基于深度学习的算法现在已经使得在数百维甚至更高维度中解决大类PDEs变得相当常见[6]，而这在几年前是不可能的。在另一个方向上，神经网络还可以用于参数化解决方案。

尽管付出了大量努力并取得了巨大成功，但解决PDEs的情况甚至对于一些传统工程问题来说仍然不完全令人满意。以下是我们仍然遇到的一些困难的不完全列表：

复杂几何问题。典型问题是多孔介质中的斯托克斯流[1]。原则上，有限元方法（FEM）非常适用于具有复杂几何形状的问题。但在实际中，找到一个合适的网格通常是一项非常复杂的任务，无论从人力投入还是实际计算成本的角度来看。基于机器学习的算法虽然容易编码，但在实际情况中尚未证明在与传统算法的竞争中具备可靠性。

动力学方程。尽管其维度远低于上述高维问题，但动力学方程，如玻尔兹曼方程，传统上被视为高维问题，传统方法在处理这些问题时确实遇到了困难。基于稀疏网格的思想应该有所帮助[2, 21]，但目前解决动力学方程的最流行方法仍然是直接模拟蒙特卡洛算法（DSMC）[23]。DSMC的一个问题是其产生的解包含太多噪音。

多尺度问题。示例包括涉及化学动力学的问题，通常涵盖了大范围的时间尺度；完全发展的湍流流动包含大范围的空间和时间尺度；以及复合材料的建模；参见[4]。

本文的目标是提出一种解决通用PDEs的方法，该方法既具有传统方法的优点，又兼具基于机器学习的算法的优势。这一新类算法可以实现谱精度。同时，它们也是无网格的，因此即使在具有复杂几何形状的情况下，也易于使用。我们的出发点是基于一系列相当简单而众所周知的思想的组合：我们使用随机特征函数来表示近似解，用最小二乘法处理PDE以及边界条件，并采用重新缩放程序来平衡损失函数中来自PDE和边界条件的贡献。在实际实现中，我们汲取了机器学习文献中的一些灵感。

经典数值方法：
线性系统往往行列数相等（条件个数=自由度个数）
通常具有稳定的收敛阶（在log-log误差图中误差线性下降）
依赖网格，难以处理复杂的计算区域
无法求解高维问题（存在维数灾难）
无法求解反问题（不可微分框架）

机器学习方法：
线性系统行列数可以不相等（条件个数≠自由度个数）
不依赖网格，在处理复杂区域时有显著优势
能够求解极其高维的微分方程，甚至可以用于解算子的参数化
可微分框架，能够在同一框架下求解反问题
需要长时训练，方程求解时间长
非凸优化导致方程数值解的精度有限，无法系统性优化
边界罚项中罚参数的调整困难

随机特征方法（RFM）：
线性系统行列数可以不相等（条件个数≠自由度个数）
具有谱精度（在semi-log误差图中误差线性下降）
不依赖网格，在处理复杂区域时有显著优势
线性最小二乘优化框架，求解精度高、效率高
可微分框架，能够在同一框架下求解反问题
边界罚项中罚参数的调整容易

\subsection{RFM解偏微分方程}
随机特征方法（random feature metod, RFM）是\cite{rfm}提出的一种用于解偏微分方程组的方法，该方法是传统算法和基于机器学习的算法之间的自然桥梁，它吸取了二者的优点，将微分方程求解问题转换为线性最小二乘问题。RFM基于以下几个思想的组合：
\begin{enumerate}
	\item 使用随机特征函数表示近似解；
	\item 用配点法处理偏微分方程的约束；
	\item 用罚函数处理边界条件，这使得我们可以在相同的基础上处理边界条件和物理方程的约束；
	\item 多尺度表示；
	\item 损失函数中各项权重的重新缩放，以平衡各项对总损失的影响；
\end{enumerate}

考虑如下静态边值问题：
\begin{equation}
	\begin{cases}
		\mathcal{L}u(\boldsymbol{x}) = f(\boldsymbol{x})  \quad \boldsymbol{x}  \in \Omega \\ \label{static-pde}
		\mathcal{B}u(\boldsymbol{x}) = g(\boldsymbol{x})  \quad \boldsymbol{x} \in \partial\Omega
	\end{cases}
\end{equation}

其中，$f$和$g$是已知函数，$\boldsymbol{x} = (x_1,x_2,\cdots,x_{d_x})^T  \in \Omega \subset \mathbb{R}^{d_x}$，$\partial\Omega$是$\Omega$的边界，记$d_x \in \mathbb{N}^+$ 为$\boldsymbol{x}$的维度，$d_u \in \mathbb{N}^+$ 为输出的维度，$\mathcal{L}$是线性微分算子，$\mathcal{B}$是边界算子。

\subsubsection{随机特征函数}
随机特征函数（random feature function, RFF）就是特征向量随机生成的函数。参考神经网络的随机特征模型，选取随机特征函数作为基函数，构造一个内部参数固定的三层神经网络。{\color{red}如图}，输入层和隐藏层之间的权重参数和偏置参数固定，隐藏层和输出层之间的权重参数待训练。

对于机器学习方法来说，特征向量就是网络的权重和偏置，特征函数的随机生成就是网络权重和偏置的随机初始化步骤。因此从机器学习的框架下看，RFM就是利用 $M$ 个定义在 $\Omega$ 上的网络基函数 $\{\phi_m\}$ 的线性组合来表示数值解：
\begin{equation}
	u_M(\boldsymbol{x}) = \sum_{m=1}^M u_m \phi_m(\boldsymbol{x})
\end{equation}
\begin{equation}
	\phi_m(\boldsymbol{x}) = \sigma(\boldsymbol{k}_m \cdot\boldsymbol{x} + b_m)
\end{equation}

其中 $\boldsymbol{k}_m, b_m$ 就是随机生成后固定的内层参数，而 $\sigma$ 是非线性激活函数（一般取双曲正切函数$tanh$即可），为方程求解提供非线性的部分；$u_m$是外层待训练的参数。

此时，对于$\forall\boldsymbol{x}  \in \Omega$，则有：

\begin{equation*}
	\begin{aligned}
		 \mathcal{L}u(\boldsymbol{x}) &\approx \mathcal{L}u_M(\boldsymbol{x}) \\
		&= \mathcal{L}\sum_{m=1}^M u_m \phi_m(\boldsymbol{x}) \\
		&=\sum_{m=1}^M u_m \mathcal{L}\phi_m(\boldsymbol{x}) \\
		&=
		\begin{bmatrix}
			\mathcal{L}\phi_1(\boldsymbol{x}) & \mathcal{L}\phi_2(\boldsymbol{x})& \cdots & \mathcal{L}\phi_M(\boldsymbol{x})
		\end{bmatrix}
		\begin{bmatrix}
			u_1 \\
			u_2 \\
			\vdots \\
			u_M
		\end{bmatrix}
	\end{aligned}
\end{equation*}

所以，我们在$\Omega$上取一组配点$\boldsymbol{x}^{(1)}_P, \boldsymbol{x}^{(2)}_P, \cdots , \boldsymbol{x}^{(N_P)}_P$，则有

\begin{equation}\label{rfm:pde-linear}
	\begin{bmatrix}
		\mathcal{L}\phi_1(\boldsymbol{x}^{(1)}_P) & \mathcal{L}\phi_2(\boldsymbol{x}^{(1)}_P)& \cdots & \mathcal{L}\phi_M(\boldsymbol{x}^{(1)}_P) \\
		\mathcal{L}\phi_1(\boldsymbol{x}^{(2)}_P) & \mathcal{L}\phi_2(\boldsymbol{x}^{(2)}_P)& \cdots & \mathcal{L}\phi_M(\boldsymbol{x}^{(2)}_P) \\
		\vdots & \vdots & \ddots & \vdots \\
		\mathcal{L}\phi_1(\boldsymbol{x}^{(N_P)}_P) & \mathcal{L}\phi_2(\boldsymbol{x}^{(N_P)}_P)& \cdots & \mathcal{L}\phi_M(\boldsymbol{x}^{(N_P)}_P)
	\end{bmatrix}
	\begin{bmatrix}
		u_1 \\
		u_2 \\
		\vdots \\
		u_M
	\end{bmatrix}
	\approx
	\begin{bmatrix}
		f(\boldsymbol{x}^{(1)}_P) \\
		f(\boldsymbol{x}^{(2)}_P) \\
		\vdots \\
		f(\boldsymbol{x}^{(N_P)}_P)
	\end{bmatrix}
\end{equation}

同理，我们在$\partial\Omega$上取一组配点$\boldsymbol{x}^{(1)}_B, \boldsymbol{x}^{(2)}_B, \cdots , \boldsymbol{x}^{(N_B)}_B$，则有

\begin{equation}\label{rfm:bc-linear}
	\begin{bmatrix}
		\phi_1(\boldsymbol{x}^{(1)}_B) & \phi_2(\boldsymbol{x}^{(1)}_B)& \cdots & \phi_M(\boldsymbol{x}^{(1)}_B) \\
		\phi_1(\boldsymbol{x}^{(2)}_B) & \phi_2(\boldsymbol{x}^{(2)}_B)& \cdots & \phi_M(\boldsymbol{x}^{(2)}_B) \\
		\vdots & \vdots & \ddots & \vdots \\
		\phi_1(\boldsymbol{x}^{(N_B)}_B) & \phi_2(\boldsymbol{x}^{(N_B)}_B)& \cdots & \phi_M(\boldsymbol{x}^{(N_B)}_B)
	\end{bmatrix}
	\begin{bmatrix}
		u_1 \\
		u_2 \\
		\vdots \\
		u_M
	\end{bmatrix}
	\approx
	\begin{bmatrix}
		g(\boldsymbol{x}^{(1)}_B) \\
		g(\boldsymbol{x}^{(2)}_B) \\
		\vdots \\
		g(\boldsymbol{x}^{(N_B)}_B)
	\end{bmatrix}
\end{equation}

记：
\begin{equation*}
	P = 
	\begin{bmatrix}
		\mathcal{L}\phi_1(\boldsymbol{x}^{(1)}_P) & \mathcal{L}\phi_2(\boldsymbol{x}^{(1)}_P)& \cdots & \mathcal{L}\phi_M(\boldsymbol{x}^{(1)}_P) \\
		\mathcal{L}\phi_1(\boldsymbol{x}^{(2)}_P) & \mathcal{L}\phi_2(\boldsymbol{x}^{(2)}_P)& \cdots & \mathcal{L}\phi_M(\boldsymbol{x}^{(2)}_P) \\
		\vdots & \vdots & \ddots & \vdots \\
		\mathcal{L}\phi_1(\boldsymbol{x}^{(N_P)}_P) & \mathcal{L}\phi_2(\boldsymbol{x}^{(N_P)}_P)& \cdots & \mathcal{L}\phi_M(\boldsymbol{x}^{(N_P)}_P)
	\end{bmatrix}
\end{equation*}

\begin{equation*}
	B = 
	\begin{bmatrix}
		\phi_1(\boldsymbol{x}^{(1)}_B) & \phi_2(\boldsymbol{x}^{(1)}_B)& \cdots & \phi_M(\boldsymbol{x}^{(1)}_B) \\
		\phi_1(\boldsymbol{x}^{(2)}_B) & \phi_2(\boldsymbol{x}^{(2)}_B)& \cdots & \phi_M(\boldsymbol{x}^{(2)}_B) \\
		\vdots & \vdots & \ddots & \vdots \\
		\phi_1(\boldsymbol{x}^{(N_B)}_B) & \phi_2(\boldsymbol{x}^{(N_B)}_B)& \cdots & \phi_M(\boldsymbol{x}^{(N_B)})
	\end{bmatrix}
\end{equation*}

\begin{equation*}
	u = 
	\begin{bmatrix}
		u_1 \\
		u_2 \\
		\vdots \\
		u_M
	\end{bmatrix}
	\quad , f = 
	\begin{bmatrix}
		f(\boldsymbol{x}^{(1)}_P) \\
		f(\boldsymbol{x}^{(2)}_P) \\
		\vdots \\
		f(\boldsymbol{x}^{(N_P)}_P)\\
		g(\boldsymbol{x}^{(1)}_B) \\
		g(\boldsymbol{x}^{(2)}_B) \\
		\vdots \\
		g(\boldsymbol{x}^{(N_B)}_B)
	\end{bmatrix}
\end{equation*}

将方程\ref{rfm:pde-linear}和\ref{rfm:bc-linear} 整合成一个线性方程组：

\begin{equation}\label{rfm:linear}
	Au = f \quad , A = 
	\begin{bmatrix}
		P \\
		B \\
	\end{bmatrix}
\end{equation}

这样一来就把偏微分方程的求解问题转换成了形如 $Au = f$ 的线性方程组求解问题，其中$u$是待求解的参数。这种问题可以采用线性最小二乘方法，当前比较流行的科学计算库都有相应的API可以直接使用，如Numpy、Scipy等。

\subsubsection{局部随机特征模型和单位分解}
上述随机特征函数是全局定义的，一般而言是无法较好地拟合偏微分方程的，因为微分方程的解常常有小尺度的局部变化。这一点从神经网络的角度去理解也是很显然的，用一个三层的神经网络去拟合一个较为复杂的偏微分方程，并且还是在内层参数固定，可训练参数只有一层的情况下，几乎是无法较好地拟合的。为了解决这一问题，可以将定义域划分为多个局部子区域，让每个局部子区域的范围足够小，并在每个局部子区域构造随机特征函数，使得每个局部随机特征函数能够很好地拟合对应范围内的偏微分方程。最后再用单位分解（partition of unity, PoU）技术将它们组合。

具体来说，分为以下几个步骤：
\begin{enumerate}
	\item 将定义域$\Omega$划分为多个子区域$\{\Omega_n\}^{M_p}_{n=1}$，在没有先验知识的情况下，均匀划分即可。
	
	\item 计算所有子区域的的中心$\{\hat{\boldsymbol{x}}_n\}^{M_p}_{n=1}$和区域半径$\{\boldsymbol{r}_n\}^{M_p}_{n=1}$。
	\begin{equation}
		\begin{aligned}
			\hat{\boldsymbol{x}}_n &= (\hat{x}_{n1}, \hat{x}_{n2}, \cdots ,\hat{x}_{nd_x})^T \\
			&= (\frac{x_{n1\_max} +  x_{n1\_min}}{2}, \frac{x_{n2\_max} +  x_{n2\_min}}{2}, \cdots, \frac{x_{nd_x\_max} +  x_{nd_x\_min}}{2})^T
		\end{aligned}
	\end{equation}
	
	\begin{equation}
		\begin{aligned}
			\boldsymbol{r}_n &= (r_{n1}, r_{n2}, \cdots ,r_{nd_x})^T \\
			&= (\frac{x_{n1\_max} -  x_{n1\_min}}{2}, \frac{x_{n2\_max} -  x_{n2\_min}}{2}, \cdots, \frac{x_{nd_x\_max} -  x_{nd_x\_min}}{2})^T
		\end{aligned}
	\end{equation}
	其中，$x_{nj\_max}$和$x_{nj\_min}$是子区域$\Omega_n$在第$j$维的区间上限和区间下限。
	
	\item 在每个 $\Omega_n$ 上构造线性变换
	\begin{equation}
		l_n(\boldsymbol{x})=\frac{1}{\boldsymbol{r}_{n}}(\boldsymbol{x}-\hat{\boldsymbol{x}}_n), \quad n=1, 2, \cdots, M_p, \quad \boldsymbol{x} \in \Omega
	\end{equation}
	这个线性变换类似于神经网络的输入标准化，将 $[\hat{x}_{n1}-r_{n1}, \hat{x}_{n1}+r_{n1}] \times [\hat{x}_{n2}-r_{n2}, \hat{x}_{n2}+r_{n2}] \times \cdots \times[\hat{x}_{nd_x}-r_{nd_x}, \hat{x}_{nd_x}+r_{nd_x}]$ 的小局部映射到统一的区间 $[-1,1]^{d_x}$，以便实现局部特征的拟合。
	
	\item 对每个$\Omega_n$，通过一个两层的神经网络构造$J_n \in \mathbb{N}^+$个随机特征函数：
	\begin{equation}\label{eqn:basis0}
		\phi_{nj}(\boldsymbol{x}) = \sigma(\boldsymbol{k}_{nj} \boldsymbol{x} + b_{nj}), \quad j=1, \cdots, J_n.
	\end{equation}
	其中，非线性激活函数$\sigma$一般取为双曲正切函数$tanh$或三角函数$sin, cos$；$\boldsymbol{k}_{nj}$和$b_{nj}$独立同分布，都是均匀采样：$\boldsymbol{k}_{nj} \sim \mathbb{U}([-R_{nj}, R_{nj}]^{d_x})$ 和 $b_{nj} \sim \mathbb{U}([-R_{nj}, R_{nj}])$，一旦随机初始化完成，则固定不变。
	
	\item 以每个$\Omega_n$为支集，构造一个单位分解函数$\psi_n$，即$supp(\psi_n) = \Omega_n$。单位分解函数一般有两种取法，以下以$d_x = 1$，即一维输入的情形举例，函数图像{\color{red} 见图}：
	\begin{subequations}
		\begin{equation}
			\psi_{n}^{a}(x)=\mathbb{I}_{[-1, 1]}(x)
	\end{equation}
		\begin{equation}
			\psi_{n}^{b}(x) =
			\begin{cases}
				\frac{1+\sin (2 \pi x)}{2} \quad  -\frac{5}{4} \leq x < -\frac{3}{4},\\
				1 \qquad \qquad -\frac{3}{4} \leq x < \frac{3}{4},\\
				\frac{1-\sin (2 \pi x)}{2} \quad \frac{3}{4} \leq x < \frac{5}{4},\\
				0 \qquad \qquad otherwise.
			\end{cases}
		\end{equation}
	\end{subequations}
	对于$d_x \ge 2$，即多维输入的情形。先对每个维度构造一维的单位分解函数，然后对这些一维单位分解函数进行张量积运算：$\psi_n(\boldsymbol{x}) = \prod^{d_x}_{i=1} \psi_n(x_i)$。
	
	则最终数值解就是将局部随机特征函数通过单位分解函数组合起来：
	\begin{equation}
		u_M(\boldsymbol{x})=\sum_{n=1}^{M_p} \psi_n ( l_n(\boldsymbol{x}))   \sum_{j=1}^{J_n }u_{nj} \phi_{nj} ( l_n(\boldsymbol{x}))
	\end{equation}
	
	需要注意的是，在单位分解函数的连续性不满足方程解所需的连续性时，需要额外加入一组连续性条件。例如：在使用$\psi^{a}$求解二阶方程时，需要在不同单位分解计算区域的接口处加入零阶和一阶连续性条件。而$\psi^{b}$是连续可微的，不需要添加额外条件。两种单位分解函数并无优劣之分，后文如无特殊说明，皆采用$\psi^{b}$作为默认的单位分解函数。
\end{enumerate}

仍以方程\ref{static-pde}为例，对其使用局部随机特征模型和单位分解技术进行离散化：此时，对于$\forall\boldsymbol{x}  \in \Omega$，则有：

\begin{equation}\label{rfm-operator-discretization}
	\begin{aligned}
		 \mathcal{L}u(\boldsymbol{x}) &\approx \mathcal{L}u_M(\boldsymbol{x}) \\
		&= \mathcal{L}\sum_{n=1}^{M_p} \psi_n ( l_n(\boldsymbol{x}))   \sum_{j=1}^{J_n }u_{nj} \phi_{nj} ( l_n(\boldsymbol{x})) \\
		&= \mathcal{L}\sum_{n=1}^{M_p} \sum_{j=1}^{J_n }u_{nj}  \psi_n ( l_n(\boldsymbol{x})) \phi_{nj} ( l_n(\boldsymbol{x})) \\
		&= \sum_{n=1}^{M_p} \sum_{j=1}^{J_n }u_{nj} \mathcal{L} \psi_n ( l_n(\boldsymbol{x})) \phi_{nj} ( l_n(\boldsymbol{x})) \\
		&= \sum_{n=1}^{M_p} \sum_{j=1}^{J_n }u_{nj} \mathcal{L} \Phi_{nj}(\boldsymbol{x}) \quad \left[\Phi_{nj}(\boldsymbol{x}) \triangleq \psi_n ( l_n(\boldsymbol{x})) \phi_{nj} ( l_n(\boldsymbol{x})))  \right] \\
		&=
		\begin{bmatrix}
			\mathcal{L}\Phi_{11}(\boldsymbol{x}) & \cdots & \mathcal{L}\Phi_{1J_1}(\boldsymbol{x}) & \cdots & \mathcal{L}\Phi_{M_p1}(\boldsymbol{x}) & \cdots & \mathcal{L}\Phi_{M_pJ_{M_p}}(\boldsymbol{x})
		\end{bmatrix}
		\begin{bmatrix}
			u_{11} \\
			\vdots \\
			u_{1J_1} \\
			\vdots \\
			u_{M_p1} \\
			\vdots \\
			u_{M_pJ_{M_p}}
		\end{bmatrix}
	\end{aligned}
\end{equation}

所以，我们在$\Omega$上取一组配点$\boldsymbol{x}^{(1)}_P, \boldsymbol{x}^{(2)}_P, \cdots , \boldsymbol{x}^{(N_P)}_P$，则有

\begin{equation}
	\scalebox{0.8}{$
	\begin{bmatrix}
		\mathcal{L}\Phi_{11}(\boldsymbol{x}^{(1)}_P) & \cdots & \mathcal{L}\Phi_{1J_1}(\boldsymbol{x}^{(1)}_P) & \cdots & \mathcal{L}\Phi_{M_p1}(\boldsymbol{x}^{(1)}_P) & \cdots & \mathcal{L}\Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(1)}_P) \\
		\mathcal{L}\Phi_{11}(\boldsymbol{x}^{(2)}_P) & \cdots & \mathcal{L}\Phi_{1J_1}(\boldsymbol{x}^{(2)}_P) & \cdots & \mathcal{L}\Phi_{M_p1}(\boldsymbol{x}^{(2)}_P) & \cdots & \mathcal{L}\Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(2)}_P) \\
		\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
		\mathcal{L}\Phi_{11}(\boldsymbol{x}^{(N_P)}_P) & \cdots & \mathcal{L}\Phi_{1J_1}(\boldsymbol{x}^{(N_P)}_P) & \cdots & \mathcal{L}\Phi_{M_p1}(\boldsymbol{x}^{(N_P)}_P) & \cdots & \mathcal{L}\Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(N_P)}_P)
	\end{bmatrix}
	\begin{bmatrix}
		u_{11} \\
		\vdots \\
		u_{1J_1} \\
		\vdots \\
		u_{M_p1} \\
		\vdots \\
		u_{M_pJ_{M_p}}
	\end{bmatrix}
	\approx
	\begin{bmatrix}
		f(\boldsymbol{x}^{(1)}_P) \\
		f(\boldsymbol{x}^{(2)}_P) \\
		\vdots \\
		f(\boldsymbol{x}^{(N_P)}_P)
	\end{bmatrix}
	$}
\end{equation}

同理，我们在$\partial\Omega$上取一组配点$\boldsymbol{x}^{(1)}_B, \boldsymbol{x}^{(2)}_B, \cdots , \boldsymbol{x}^{(N_B)}_B$，则有

\begin{equation}
	\scalebox{0.8}{$
	\begin{bmatrix}
		\Phi_{11}(\boldsymbol{x}^{(1)}_B) & \cdots & \Phi_{1J_1}(\boldsymbol{x}^{(1)}_B) & \cdots & \Phi_{M_p1}(\boldsymbol{x}^{(1)}_B) & \cdots & \Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(1)}_B) \\
		\Phi_{11}(\boldsymbol{x}^{(2)}_B) & \cdots & \Phi_{1J_1}(\boldsymbol{x}^{(2)}_B) & \cdots & \Phi_{M_p1}(\boldsymbol{x}^{(2)}_B) & \cdots & \Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(2)}_B) \\
		\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
		\Phi_{11}(\boldsymbol{x}^{(N_B)}_B) & \cdots & \Phi_{1J_1}(\boldsymbol{x}^{(N_B)}_B) & \cdots & \Phi_{M_p1}(\boldsymbol{x}^{(N_B)}_B) & \cdots & \Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(N_B)}_B)
	\end{bmatrix}
	\begin{bmatrix}
		u_{11} \\
		\vdots \\
		u_{1J_1} \\
		\vdots \\
		u_{M_p1} \\
		\vdots \\
		u_{M_pJ_{M_p}}
	\end{bmatrix}
	\approx
	\begin{bmatrix}
		g(\boldsymbol{x}^{(1)}_B) \\
		g(\boldsymbol{x}^{(2)}_B) \\
		\vdots \\
		g(\boldsymbol{x}^{(N_B)}_B)
	\end{bmatrix}
	$}
\end{equation}

记：
\begin{equation*}
	P = 
	\begin{bmatrix}
		\mathcal{L}\Phi_{11}(\boldsymbol{x}^{(1)}_P) & \cdots & \mathcal{L}\Phi_{1J_1}(\boldsymbol{x}^{(1)}_P) & \cdots & \mathcal{L}\Phi_{M_p1}(\boldsymbol{x}^{(1)}_P) & \cdots & \mathcal{L}\Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(1)}_P) \\
		\mathcal{L}\Phi_{11}(\boldsymbol{x}^{(2)}_P) & \cdots & \mathcal{L}\Phi_{1J_1}(\boldsymbol{x}^{(2)}_P) & \cdots & \mathcal{L}\Phi_{M_p1}(\boldsymbol{x}^{(2)}_P) & \cdots & \mathcal{L}\Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(2)}_P) \\
		\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
		\mathcal{L}\Phi_{11}(\boldsymbol{x}^{(N_P)}_P) & \cdots & \mathcal{L}\Phi_{1J_1}(\boldsymbol{x}^{(N_P)}_P) & \cdots & \mathcal{L}\Phi_{M_p1}(\boldsymbol{x}^{(N_P)}_P) & \cdots & \mathcal{L}\Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(N_P)}_P)
	\end{bmatrix}
\end{equation*}

\begin{equation*}
	B = 
	\begin{bmatrix}
		\Phi_{11}(\boldsymbol{x}^{(1)}_B) & \cdots & \Phi_{1J_1}(\boldsymbol{x}^{(1)}_B) & \cdots & \Phi_{M_p1}(\boldsymbol{x}^{(1)}_B) & \cdots & \Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(1)}_B) \\
		\Phi_{11}(\boldsymbol{x}^{(2)}_B) & \cdots & \Phi_{1J_1}(\boldsymbol{x}^{(2)}_B) & \cdots & \Phi_{M_p1}(\boldsymbol{x}^{(2)}_B) & \cdots & \Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(2)}_B) \\
		\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
		\Phi_{11}(\boldsymbol{x}^{(N_B)}_B) & \cdots & \Phi_{1J_1}(\boldsymbol{x}^{(N_B)}_B) & \cdots & \Phi_{M_p1}(\boldsymbol{x}^{(N_B)}_B) & \cdots & \Phi_{M_pJ_{M_p}}(\boldsymbol{x}^{(N_B)}_B)
	\end{bmatrix}
\end{equation*}

\begin{equation*}
	u = 
	\begin{bmatrix}
		u_{11} \\
		\vdots \\
		u_{1J_1} \\
		\vdots \\
		u_{M_p1} \\
		\vdots \\
		u_{M_pJ_{M_p}}
	\end{bmatrix}
	\quad , f = 
	\begin{bmatrix}
		f(\boldsymbol{x}^{(1)}_P) \\
		f(\boldsymbol{x}^{(2)}_P) \\
		\vdots \\
		f(\boldsymbol{x}^{(N_P)}_P)\\
		g(\boldsymbol{x}^{(1)}_B) \\
		g(\boldsymbol{x}^{(2)}_B) \\
		\vdots \\
		g(\boldsymbol{x}^{(N_B)}_B)
	\end{bmatrix}
\end{equation*}

则偏微分方程的求解问题再次转换为线性方程组求解问题，自由度为$M = \sum_{n=1}^{M_p}J_n$：
\begin{equation}
	Au = f \quad , A = 
	\begin{bmatrix}
		P \\
		B \\
	\end{bmatrix}
\end{equation}

\subsubsection{损失函数及罚参数自动缩放}
在RFM中，损失函数同样在配点上进行计算，和PINN的损失函数构造完全一致，使用强形式在配点上构造损失函数
\begin{equation}
	Loss = \sum_{\boldsymbol{x}_{i} \in \Omega}\lambda_{i}\|\mathcal{L}\boldsymbol{u_M}(\boldsymbol{x}_{i})-\boldsymbol{f}(\boldsymbol{x}_{i})\|_{2}^{2} + \sum_{\boldsymbol{x}_{j} \in \partial \Omega}\lambda_{j}\|\mathcal{B}\boldsymbol{u_M}(\boldsymbol{x}_{j})-\boldsymbol{g}(\boldsymbol{x}_{j})\|_{2}^{2}.
	\label{loss2}
\end{equation}

公式中，罚参数$\lambda_i$的确定是关键，往往决定着模型的收敛速度和收敛效果。为了平衡PDE损失项和边界条件损失项，一种简单而高效的策略就是通过缩放罚参数使得损失函数中每一项具有相同的数量级。

在传统的PINN的训练过程中，罚参数的调整较为困难，往往依赖经验。但RFM的情况则完全不同，虽然损失函数形式上看完全类似，但RFM与PINN构造的优化问题则完全不同，RFM求解的是内层参数固定、仅优化最外层线性参数的线性优化问题。从经典数值算法的角度来看，RFM构造的是线性最小二乘问题，该问题可以表为线性系统 $Au=f$ 的形式。

对于 方程$Au=f$，最小二乘解的目的就是求解向量 $u$使得误差$e = ||Au -f ||^2$达到最小，要使每一项损失$e_i = ||A_iu -f_i ||^2$具有相同的数量级，只需直接基于矩阵 $A$ 的信息，对矩阵的每一行元素$A_i$缩放到同一个数量级。

\begin{equation}\label{rfm-penalty-parameter}
	\begin{cases}
		\lambda_i = \frac{c_i}{max(A_i, -A_i)} \\
		A_i = \lambda_i \times A_i \\
		f_i = \lambda_i \times f_i
	\end{cases}
\end{equation}

其中，$max(A_i, -A_i)$表示矩阵第$i$行的最大值，$c_i$表示要缩放到的数量级，在没有先验知识的情况下设置为统一常数即可，如100。

\subsubsection{时空随机特征方法}
考虑如下时空偏微分方程：
\begin{equation}
	\begin{cases}
		\mathcal{L}u(\boldsymbol{x}, t) = f(\boldsymbol{x}, t)  \quad \boldsymbol{x}, t  \in \Omega \times [0, T] \\ \label{st-pde}
		\mathcal{B}u(\boldsymbol{x}, t) = g(\boldsymbol{x}, t)  \quad \boldsymbol{x}, t \in \partial\Omega \times [0, T] \\
		\mathcal{I}u(\boldsymbol{x}, t) = h(\boldsymbol{x})  \quad \boldsymbol{x}, t \in  \Omega
	\end{cases}
\end{equation}

其中，$f$，$g$和$h$是已知函数，$\boldsymbol{x} = (x_1,x_2,\cdots,x_{d_x})^T  \in \Omega \subset \mathbb{R}^{d_x}$，$t \in [0, T]$，$\partial\Omega$是$\Omega$的边界，记$d_x \in \mathbb{N}^+$ 为$\boldsymbol{x}$的维度，$d_u \in \mathbb{N}^+$ 为输出的维度，$\mathcal{L}$是线性微分算子，$\mathcal{B}$是边界算子，$\mathcal{I}$是初值算子。

时空随机特征方法（Space-Time Random Feature Method, ST-RFM）是一种用于求解时间相关的偏微分方程的方法，其背后思想类似于静态方程的RFM方法，下面我们简要介绍算法逻辑：
\begin{enumerate}
	\item 将空间域$\Omega$划分为$N_x$个子域$\{\Omega_n\}^{N_x}_{n=1}$，每个子域$\Omega_n$的中心点记为$\hat{\boldsymbol{x}}_n$。
	
	\item 将时间域$[0, T]$划分为$N_t$个子域，即$[0, T] = [t_0, t_1) \cup [t_1, t_2) \cup \cdots \cup [t_{N_t-1}, t_{N_t}]$，每个子域的中心点$\hat{t}_n = \frac{t_{n-1} + t_n}{2}$。
	
	\item 构造时空单位分解函数，通过对空间单位分解函数和时间单位分解函数进行乘积运算：
	\begin{equation}
		\psi_{n_x,n_t}(\boldsymbol{x}, t) = \psi_{n_x}(l_{n_x}(\boldsymbol{x})) \cdot \psi_{n_t}(l_{n_t}(t))
	\end{equation}
	其中，$n_x$和$n_t$分别是空间子域和时间子域的索引。$ l_{n_x}(\boldsymbol{x})$和$ l_{n_t}(t)$是线性変换，其作用是分别把$\boldsymbol{x} \in \Omega_{n_x}$映射到$[-1, 1]^{d_x}$，把$t \in [t_{n_t-1}, t_{n_t}]$映射到$[-1, 1]$。
	
	\item 将空间随机特征函数推广到时空随机特征函数，这里有两种思路：
	\subitem 时空拼接(concatenation of spatial and temporal, STC)，这种思路是空间随机特征函数的自然推广，相当于把空间变量$\boldsymbol{x}$和时间变量$t$拼接到一起，成为一个$d_x+1$维的输入变量。
	\begin{equation}
		\phi_{n_x,n_t,j}(\boldsymbol{x}, t) = \sigma(\boldsymbol{k}_{n_x,n_t,j} l_{n_x, n_t}(\boldsymbol{x}, t) + b_{n_x,n_t,j})
	\end{equation}
	其中，$\boldsymbol{k}_{n_x,n_t,j}$是时空输入的权重矩阵，$b_{n_x,n_t,j}$是偏置，$ l_{n_x, n_t}(\boldsymbol{x}, t)$是线性変换，其作用是把$(\boldsymbol{x}, t) \in \Omega_{n_x} \times [t_{n_t-1}, t_{n_t}]$映射到$[-1, 1]^{d_x+1}$。
	
	\subitem 变量分离(separation of variables, SoV)，为了模拟偏微分方程数值解法的一类经典方法：变量分离技术，把空间输入和时间输入对应到不同的随机特征函数，然后对结果做乘积：
	\begin{equation}
		\phi_{n_x,n_t,j}(\boldsymbol{x}, t) = \sigma(\boldsymbol{k}^x_{n_x,n_t,j} l_{n_x}(\boldsymbol{x}) + b^x_{n_x,n_t,j}) \cdot \sigma(k^t_{n_x,n_t,j} l_{n_t}(t) + b^t_{n_x,n_t,j})
	\end{equation}
	
	以上两种随机特征函数的结构{\color{red}如图}，这两种构造策略均被证明能够有效地收敛到真解，并且在理论上和数值算例上并无谁优谁劣，后文如无特殊说明，皆采用STC作为默认策略。
	
	\item 将局部随机特征函数通过单位分解函数组合起来，得到最终的数值解：
	\begin{equation}
		u_M(\boldsymbol{x}, t)=\sum_{n_x=1}^{N_x} \sum_{n_t=1}^{N_t}\psi_{n_x,n_t}(\boldsymbol{x}, t) \sum_{j=1}^{J_n }u_{n_x,n_t,j} \phi_{n_x,n_t,j}(\boldsymbol{x}, t)
	\end{equation}
	
	ST-RFM算法的其他部分诸如矩阵的构造、损失函数的罚参数缩放等，都与RFM高度相似，这里不做过多赘述。
\end{enumerate}

\section{基于ST-RFM的数据同化}
\subsection{ST-RFM的改进}
在RFM章节，公式\ref{rfm-operator-discretization}解释了微分算子离散化的过程，该过程展示了线性微分算子如何一步步转为矩阵和向量的线性运算，但该过程的结果还包含着单位分解函数，这显然并不完美。虽然文献\cite{rfm}提供了两种单位分解函数，并通过数值算例验证了其效能，但是单位分解函数的构造方式还可能有很多，无法确认哪一种才是最合适的。这里的合适有两层含义：一是能够促进数值解的收敛，在相同的参数下，尽可能达到更高的收敛阶；二是能够适合计算机的高效计算，结构不能太复杂，比如不能有太多的分支条件，否则不利于并行计算等。本文的一项工作就是对该过程做简化，去除单位分解函数，得到ST-RFM数值解的简化格式：
\begin{equation}\label{modified-rt-rfm-numerical}
	u_M(\boldsymbol{x}, t)=\sum_{n_x=1}^{N_x} \sum_{n_t=1}^{N_t} \sum_{j=1}^{J_n }u_{n_x,n_t,j} \phi_{n_x,n_t,j}(\boldsymbol{x}, t)
\end{equation}

由$\phi_{n_x,n_t,j}(\boldsymbol{x}, t)$的定义可知，其是连续可微函数。所以$u_M(\boldsymbol{x}, t)$也是连续可微的，故而不需要处理各个子区域的边界问题。这里把单位分解函数去除，虽然形式上看似变得简单了，但是在表现能力上却变得丰富了。因为经过这个改动，相当于把光滑性的构造也交给神经网络了，让神经网络自己去训练出最合适的单位分解函数。此外，由于没有了单位分解函数的代码分支逻辑，使得RFM方法的运算速度更快。在此基础上，我们也能得到微分算子离散化的简单形式：

\begin{equation}\label{modified-rt-rfm-operator}
	\begin{aligned}
		\mathcal{L}u(\boldsymbol{x}, t) &\approx \mathcal{L}u_M(\boldsymbol{x}, t) \\
		&= \mathcal{L}\sum_{n_x=1}^{N_x} \sum_{n_t=1}^{N_t} \sum_{j=1}^{J_n }u_{n_x,n_t,j} \phi_{n_x,n_t,j}(\boldsymbol{x}, t) \\
		&=\sum_{n_x=1}^{N_x} \sum_{n_t=1}^{N_t} \sum_{j=1}^{J_n } \mathcal{L} u_{n_x,n_t,j} \phi_{n_x,n_t,j}(\boldsymbol{x}, t) \\
		&=\sum_{n_x=1}^{N_x} \sum_{n_t=1}^{N_t} \sum_{j=1}^{J_n } u_{n_x,n_t,j}  \mathcal{L} \phi_{n_x,n_t,j}(\boldsymbol{x}, t)
	\end{aligned}
\end{equation}

\subsubsection{数值算例验证}
我们使用一维Helmholtz方程验证RFM，方程如下：
\begin{equation}\label{equ:Helmholtz}
	\begin{cases}
		\frac{d^2u(x)}{dx^2} - \lambda u(x) = f(x) \quad x \in [0, 8] \\
		u(0) = c_1, \quad u(8) = c_2
	\end{cases}
\end{equation}

我们取$\lambda = 4$，真解$u(x) = \sin(3\pi x + \frac{3\pi}{20}) \cos(2\pi x + \frac{\pi}{10}) + 2$，则$c_1, c_2$和$f(x)$可相应的计算出来：
\begin{equation}
	\begin{cases}
		f(x) = -13\pi^2  \sin(3\pi x + \frac{3\pi}{20}) \cos(2\pi x + \frac{\pi}{10}) -12\pi^2 \cos(3\pi x + \frac{3\pi}{20}) \sin(2\pi x + \frac{\pi}{10}) \\
		u(0) = u(8) = \sin\frac{3\pi}{20} \cos \frac{\pi}{10} + 2 
	\end{cases}
\end{equation}

对方程使用RFM进行数值求解，超参数的设置如下：

\begin{itemize}
	\item 每个子区域的特征函数的数量为$J_n=50$；
	\item 每个子区域的配点的数量为$Q_n=50$；
	\item 子区域划分的个数依次取$M_p =2, 4, 8, 16, 32 $
	\item 单位分解函数的策略分别为：$\psi^a$、$\psi^b$和无$\psi$；
\end{itemize}

计算结果见表格\ref{tab:Helmholtz}。{\color{red} 见图}，由图和表格\ref{tab:Helmholtz}，可以看出在相同超参数使用RFM求解Helmholtz方程的情况下，无$\psi$策略相比于使用$\psi^a$和$\psi^b$作为单位分解函数，收敛速度更快，并且$L^\infty$误差和$L^2$误差降低了好几个阶。数值结果验证了改进后的RFM方法对于静态方程的有效性。

\begin{table}
	\centering
	\caption{一维Helmholtz方程三种单位分解策略的比较}
	\label{tab:Helmholtz}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{$M_p$} & \multicolumn{2}{c|}{$\psi^a$} & \multicolumn{2}{c|}{$\psi^b$} & \multicolumn{2}{c|}{无$\psi$} \\
		\cline{2-7}
		& $L^\infty$ error & $L^2$ error & $L^\infty$ error & $L^2$ error & $L^\infty$ error & $L^2$ error\\
		\hline
		2 & 39.58 & 10.55 & 8.00 & 1.58 & 0.34 & 0.057\\
		\hline
		4 & 1.20E-2 & 3.09E-3  & 1.26E-4 & 1.51E-05 & 5.48E-08 & 1.11E-08\\
		\hline
		8 & 1.77E-05 & 4.50E-06  & 2.14E-07 & 3.68E-08 & 6.43E-10 & 1.04E-10\\
		\hline
		16 & 1.66E-08 & 4.75E-09  & 1.38E-09 & 2.22E-10 & 3.17E-13 & 1.46E-13\\
		\hline
		32 & 2.15E-09 & 8.69E-10  & 1.27E-10 & 2.64E-11 & 3.89E-14 & 1.31E-14\\
		\hline
	\end{tabular}
\end{table}

我们使用一维扩散方程验证ST-RFM，方程如下：
\begin{equation}\label{equ:Diffusion}
	\begin{cases}
		\frac{\partial u}{\partial t} - \nu \frac{\partial^2 u}{\partial x^2} = f(x, t)  \quad (x, t) \in [0, 5] \times [0, 1] \\
		u(0, t) = g_1(t) \quad t \in [0, 1] \\
		u(5, t) = g_2(t) \quad t \in [0, 1] \\
		u(x, 0) = h(x) \quad x \in [0, 5] 
	\end{cases}
\end{equation}

我们取定真解为$u(x, t) = [2\cos(\pi x + \frac{\pi}{5}) + \frac{3}{2}\cos(2\pi x - \frac{3\pi}{5})] [2\cos(\pi t + \frac{\pi}{5}) + \frac{3}{2}\cos(2\pi t - \frac{3\pi}{5})]$，则$f(x,t), g_1(t), g_2(t), h(x)$可相应的计算出来：
\begin{equation}
	\begin{cases}
		f(x, t) &= [2\cos(\pi x + \frac{\pi}{5}) + \frac{3}{2}\cos(2\pi x - \frac{3\pi}{5})] [-2\pi\sin(\pi t + \frac{\pi}{5}) -3\pi\sin(2\pi t - \frac{3\pi}{5}) ]\\
		 & +\nu [2\pi^2 \cos(\pi x + \frac{\pi}{5}) + 6\pi^2 \cos(2\pi x -\frac{3\pi}{5})] [2\cos(\pi t + \frac{\pi}{5}) + \frac{3}{2}\cos(2\pi t - \frac{3\pi}{5})]\\
		g_1(t) &= [2\cos\frac{\pi}{5} + \frac{3}{2}\cos\frac{3\pi}{5}] [2\cos(\pi t + \frac{\pi}{5}) + \frac{3}{2}\cos(2\pi t - \frac{3\pi}{5})] \\
		g_2(t) &= [-2\cos\frac{\pi}{5} + \frac{3}{2}\cos\frac{3\pi}{5}] [2\cos(\pi t + \frac{\pi}{5}) + \frac{3}{2}\cos(2\pi t - \frac{3\pi}{5})] \\
		h(x) &= [2\cos(\pi x + \frac{\pi}{5}) + \frac{3}{2}\cos(2\pi x - \frac{3\pi}{5})] [2\cos \frac{\pi}{5} + \frac{3}{2}\cos \frac{3\pi}{5}] 
	\end{cases}
\end{equation}
其中，扩散系数取$\nu=0.01$。

对方程使用ST-RFM进行数值求解，超参数的设置如下：

\begin{itemize}
	\item 空间维度的划分数量为$N_x = 5$；
	\item 时间维度的划分数量为$N_t = 2$；
	\item 每个子区域的空间维度的配点数量为$Q_x = 30$；
	\item 每个子区域的时间维度的配点数量为$Q_t = 30$；
	\item 每个子区域的特征函数的数量依次取$J_n=50, 100, 150, 200, 250$；
	\item 单位分解函数的策略分别为：$\psi^a$、$\psi^b$和无$\psi$；
\end{itemize}

所以每个子区域的配点数量为$Q_n = Q_x \times Q_t = 900$，求解结果见表格\ref{tab:Diffusion}。{\color{red} 见图，Jn为横坐标，损失为纵坐标画折线图}，可见无$\psi$的策略下收敛速度更快，在同等超参数的情况下相较于$\psi^a$和$\psi^b$，误差小几个数量级。

通过数值算例可以看到，去除单位分解函数，确实大大提高了训练的收敛速度和收敛精度。{\color{red} 这里可以补充下训练时间，无psi的情况训练时间应该也是快一点的}

\begin{table}
	\centering
	\caption{一维Diffusion方程三种单位分解策略的比较}
	\label{tab:Diffusion}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{$J_n$} & \multicolumn{2}{c|}{$\psi^a$} & \multicolumn{2}{c|}{$\psi^b$} & \multicolumn{2}{c|}{无$\psi$} \\
		\cline{2-7}
		& $L^\infty$ error & $L^2$ error & $L^\infty$ error & $L^2$ error & $L^\infty$ error & $L^2$ error\\
		\hline
		50 & 6.60E-02 & 1.64E-02 & 7.82E-02 & 1.58E-02 & 2.37E-04 & 2.18E-05\\
		\hline
		100 & 5.70E-04 & 7.17E-05 & 4.83E-04 & 5.87E-05 & 1.26E-07 & 1.26E-08\\
		\hline
		150 &8.89E-06 & 9.73E-07 & 1.29E-05 & 1.77E-06 & 3.51E-09 & 2.78E-10\\
		\hline
		200 & 3.46E-07 & 3.93E-08 & 1.54E-06 & 2.03E-07 & 9.48E-10 & 8.54E-11\\
		\hline
		250 & 4.21E-08 & 6.46E-09 & 2.14E-07 & 2.97E-08 & 3.59E-10 & 6.41E-11\\
		\hline
	\end{tabular}
\end{table}

\subsection{使用改进的ST-RFM做数据同化}
由于改进后的ST-RFM具有更快的收敛速度和收敛阶，所以我们基于改进的ST-RFM做数据同化。

考虑如下数据同化问题：
\begin{equation}
	\begin{cases}
		\mathcal{L}u(\boldsymbol{x}, t) = f(\boldsymbol{x}, t)  \quad \boldsymbol{x},t  \in \Omega \times [0, T] \\
		\mathcal{B}u(\boldsymbol{x},t) = g(\boldsymbol{x},t)  \quad \boldsymbol{x},t \in \Omega \times [0,T]\\
		\mathcal{I}u(\boldsymbol{x},0) = s(\boldsymbol{x})  \qquad \boldsymbol{x} \in \Omega\\
		\mathcal{H}u(\boldsymbol{x},t) = h(\boldsymbol{x},t)  \quad \boldsymbol{x},t \in \Omega \times [0,T]\\
	\end{cases}
\end{equation}

其中，$\Omega$是空间域，$[0, T]$是时间域，$\mathcal{L}$是线性微分算子，$\mathcal{B}$是边界算子，$\mathcal{I}$是初值算子，这三者构成动力系统。$\mathcal{H}$是观测算子，一般在空间的某些固定位置，进行连续或间断观测。接下来，本文基于随机特征方法，解决这一类数据同化问题。步骤如下：

\begin{enumerate}
	\item 求解区域离散化
	
	将空间域$\Omega$划分为$N_x$个子域$\{\Omega_n\}^{N_x}_{n=1}$，每个子域$\Omega_n$的中心点记为$\hat{\boldsymbol{x}}_n$。将时间域$[0, T]$划分为$N_t$个子域，即$[0, T] = [t_0, t_1) \cup [t_1, t_2) \cup \cdots \cup [t_{N_t-1}, t_{N_t}]$，每个子域的中心点$\hat{t}_n = \frac{t_{n-1} + t_n}{2}$。则总的区域数为$M_p=N_x \cdot N_t$，对第$n$个区域随机初始化$J_n$个随机特征函数。
	
	\item 微分算子离散化
	
	利用改进的ST-RFM微分算子离散化公式\ref{modified-rt-rfm-operator}，可得：
	\begin{equation}
		\begin{aligned}
			f(\boldsymbol{x}, t) &= \mathcal{L}u(\boldsymbol{x}, t) \\
			&\approx \mathcal{L}u_M(\boldsymbol{x}, t) \\
			&= \sum_{n_x=1}^{N_x} \sum_{n_t=1}^{N_t} \sum_{j=1}^{J_n } u_{n_x,n_t,j}  \mathcal{L} \phi_{n_x,n_t,j}(\boldsymbol{x}, t) \\
			&= \sum_{n=1}^{M_p} \sum_{j=1}^{J_n } u_{nj}  \mathcal{L} \phi_{nj}(\boldsymbol{y}) \quad \left[\boldsymbol{y} \triangleq (\boldsymbol{x}, t) \right]
		\end{aligned}
	\end{equation}

	我们在$\Omega \times [0, T] $上取一组配点$\boldsymbol{y}^{(1)}_P, \boldsymbol{y}^{(2)}_P, \cdots , \boldsymbol{y}^{(N_P)}_P$，则有
	\begin{equation}
		\scalebox{0.8}{$
			\begin{bmatrix}
				\mathcal{L}\phi_{11}(\boldsymbol{y}^{(1)}_P) & \cdots & \mathcal{L}\phi_{1J_1}(\boldsymbol{y}^{(1)}_P) & \cdots & \mathcal{L}\phi_{M_p1}(\boldsymbol{y}^{(1)}_P) & \cdots & \mathcal{L}\phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(1)}_P) \\
				\mathcal{L}\phi_{11}(\boldsymbol{y}^{(2)}_P) & \cdots & \mathcal{L}\phi_{1J_1}(\boldsymbol{y}^{(2)}_P) & \cdots & \mathcal{L}\phi_{M_p1}(\boldsymbol{y}^{(2)}_P) & \cdots & \mathcal{L}\phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(2)}_P) \\
				\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
				\mathcal{L}\phi_{11}(\boldsymbol{y}^{(N_P)}_P) & \cdots & \mathcal{L}\phi_{1J_1}(\boldsymbol{y}^{(N_P)}_P) & \cdots & \mathcal{L}\phi_{M_p1}(\boldsymbol{y}^{(N_P)}_P) & \cdots & \mathcal{L}\phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(N_P)}_P)
			\end{bmatrix}
			\begin{bmatrix}
				u_{11} \\
				\vdots \\
				u_{1J_1} \\
				\vdots \\
				u_{M_p1} \\
				\vdots \\
				u_{M_pJ_{M_p}}
			\end{bmatrix} 
			\approx
			\begin{bmatrix}
				f(\boldsymbol{y}^{(1)}_P) \\
				f(\boldsymbol{y}^{(2)}_P) \\
				\vdots \\
				f(\boldsymbol{y}^{(N_P)}_P)
			\end{bmatrix}
			$}
	\end{equation}

	把上述公式简记为：$Pu = f$
	
	
	\item 边界算子离散化
	
	我们在$\partial\Omega \times [0, T]$上取一组配点$\boldsymbol{y}^{(1)}_B, \boldsymbol{y}^{(2)}_B, \cdots , \boldsymbol{y}^{(N_B)}_B$，则有
	\begin{equation}
		\scalebox{0.8}{$
			\begin{bmatrix}
				\phi_{11}(\boldsymbol{y}^{(1)}_B) & \cdots & \phi_{1J_1}(\boldsymbol{y}^{(1)}_B) & \cdots & \phi_{M_p1}(\boldsymbol{y}^{(1)}_B) & \cdots & \phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(1)}_B) \\
				\phi_{11}(\boldsymbol{y}^{(2)}_B) & \cdots & \phi_{1J_1}(\boldsymbol{y}^{(2)}_B) & \cdots & \phi_{M_p1}(\boldsymbol{y}^{(2)}_B) & \cdots & \phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(2)}_B) \\
				\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
				\phi_{11}(\boldsymbol{y}^{(N_B)}_B) & \cdots & \phi_{1J_1}(\boldsymbol{y}^{(N_B)}_B) & \cdots & \phi_{M_p1}(\boldsymbol{y}^{(N_B)}_B) & \cdots & \phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(N_B)}_B)
			\end{bmatrix}
			\begin{bmatrix}
				u_{11} \\
				\vdots \\
				u_{1J_1} \\
				\vdots \\
				u_{M_p1} \\
				\vdots \\
				u_{M_pJ_{M_p}}
			\end{bmatrix}
			\approx
			\begin{bmatrix}
				g(\boldsymbol{y}^{(1)}_B) \\
				g(\boldsymbol{y}^{(2)}_B) \\
				\vdots \\
				g(\boldsymbol{y}^{(N_B)}_B)
			\end{bmatrix}
			$}
	\end{equation}

	把上述公式简记为：$Bu = g$
	
	\item 初值算子离散化
	
	我们在$\Omega \times [0]$上取一组配点$\boldsymbol{y}^{(1)}_I, \boldsymbol{y}^{(2)}_I, \cdots , \boldsymbol{y}^{(N_I)}_I$，则有
	\begin{equation}
		\scalebox{0.8}{$
			\begin{bmatrix}
				\phi_{11}(\boldsymbol{y}^{(1)}_I) & \cdots & \phi_{1J_1}(\boldsymbol{y}^{(1)}_I) & \cdots & \phi_{M_p1}(\boldsymbol{y}^{(1)}_I) & \cdots & \phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(1)}_I) \\
				\phi_{11}(\boldsymbol{y}^{(2)}_I) & \cdots & \phi_{1J_1}(\boldsymbol{y}^{(2)}_I) & \cdots & \phi_{M_p1}(\boldsymbol{y}^{(2)}_I) & \cdots & \phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(2)}_I) \\
				\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
				\phi_{11}(\boldsymbol{y}^{(N_I)}_I) & \cdots & \phi_{1J_1}(\boldsymbol{y}^{(N_I)}_I) & \cdots & \phi_{M_p1}(\boldsymbol{y}^{(N_I)}_I) & \cdots & \phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(N_I)}_I)
			\end{bmatrix}
			\begin{bmatrix}
				u_{11} \\
				\vdots \\
				u_{1J_1} \\
				\vdots \\
				u_{M_p1} \\
				\vdots \\
				u_{M_pJ_{M_p}}
			\end{bmatrix}
			\approx
			\begin{bmatrix}
				s(\boldsymbol{y}^{(1)}_I) \\
				s(\boldsymbol{y}^{(2)}_I) \\
				\vdots \\
				s(\boldsymbol{y}^{(N_I)}_I)
			\end{bmatrix}
			$}
	\end{equation}
	
	把上述公式简记为：$Iu = s$
	
	\item 观测算子离散化
	
	我们在$\Omega \times [0, T]$上取一组观测数据$\boldsymbol{y}^{(1)}_H, \boldsymbol{y}^{(2)}_H, \cdots , \boldsymbol{y}^{(N_H)}_H$，则有
	\begin{equation}
		\scalebox{0.8}{$
			\begin{bmatrix}
				\phi_{11}(\boldsymbol{y}^{(1)}_H) & \cdots & \phi_{1J_1}(\boldsymbol{y}^{(1)}_H) & \cdots & \phi_{M_p1}(\boldsymbol{y}^{(1)}_H) & \cdots & \phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(1)}_H) \\
				\phi_{11}(\boldsymbol{y}^{(2)}_H) & \cdots & \phi_{1J_1}(\boldsymbol{y}^{(2)}_H) & \cdots & \phi_{M_p1}(\boldsymbol{y}^{(2)}_H) & \cdots & \phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(2)}_H) \\
				\vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
				\phi_{11}(\boldsymbol{y}^{(N_H)}_H) & \cdots & \phi_{1J_1}(\boldsymbol{y}^{(N_H)}_H) & \cdots & \phi_{M_p1}(\boldsymbol{y}^{(N_H)}_H) & \cdots & \phi_{M_pJ_{M_p}}(\boldsymbol{y}^{(N_H)}_H)
			\end{bmatrix}
			\begin{bmatrix}
				u_{11} \\
				\vdots \\
				u_{1J_1} \\
				\vdots \\
				u_{M_p1} \\
				\vdots \\
				u_{M_pJ_{M_p}}
			\end{bmatrix}
			\approx
			\begin{bmatrix}
				h(\boldsymbol{y}^{(1)}_H) \\
				h(\boldsymbol{y}^{(2)}_H) \\
				\vdots \\
				h(\boldsymbol{y}^{(N_H)}_H)
			\end{bmatrix}
			$}
	\end{equation}
	
	把上述公式简记为：$Hu = h$
	
	\item 转换为线性最小二乘问题
	
	将上述方程组合并为一个大的线性方程组：
	\begin{equation}
		\begin{bmatrix}
			P \\
			B \\
			I \\
			H \\
		\end{bmatrix}
		u = 
		\begin{bmatrix}
			f \\
			g \\
			s \\
			h \\
		\end{bmatrix}
	\end{equation}

	至此，我们将数据同化的求解问题转换成了形如 $Au = f$ 的线性方程组求解问题，其中$u$是待求解的参数。这种问题可以采用线性最小二乘方法进行求解。
	
	\item 设置损失函数和罚参数
	
	损失函数在配点上进行计算，使用强形式在配点上构造损失函数：
	\begin{multline}
		Loss = \sum_{\boldsymbol{y}_{i} \in \Omega \times [0, T]}\lambda_{i}\|\mathcal{L}\boldsymbol{u_M}(\boldsymbol{y}_{i})-\boldsymbol{f}(\boldsymbol{y}_{i})\|_{2}^{2} + \sum_{\boldsymbol{y}_{j} \in \partial \Omega \times [0, T]}\lambda_{j}\|\mathcal{B}\boldsymbol{u_M}(\boldsymbol{y}_{j})-\boldsymbol{g}(\boldsymbol{y}_{j})\|_{2}^{2} +\\
		\sum_{\boldsymbol{y}_{k} \in \Omega \times [0]}\lambda_{k}\|\mathcal{I}\boldsymbol{u_M}(\boldsymbol{y}_{k})-\boldsymbol{s}(\boldsymbol{y}_{k})\|_{2}^{2} +
		\sum_{\boldsymbol{y}_{l} \in \Omega \times [0, T]}\lambda_{l}\|\mathcal{H}\boldsymbol{u_M}(\boldsymbol{y}_{l})-\boldsymbol{h}(\boldsymbol{y}_{l})\|_{2}^{2}
		\label{da-loss}
	\end{multline}
	
	其中，$\lambda_{i}, \lambda_{j}, \lambda_{k}, \lambda_{l}$是罚参数，由公式\ref{rfm-penalty-parameter}，我们知道罚参数为$\lambda_i = \frac{c_i}{max(A_i, -A_i)}$，其中$c_i$是关键的缩放因子，当没有具体的先验知识的情况下，取常数即可。但是对于数据同化问题，一般而言观测数据更为可信，所以一般会把观测损失项的罚参数的缩放因子设置的大一些，以增加观测数据的影响权重。

\end{enumerate}

我们使用一维扩散方程验证ST-RFM，方程如下：
\begin{equation}\label{equ:ConvectionDiffusion}
	\begin{cases}
		\frac{\partial u}{\partial t} + v \frac{\partial u}{\partial x} - \nu \frac{\partial^2 u}{\partial x^2} +u = f(x, t)  \quad (x, t) \in [0, 5] \times [0, 1] \\
		u(0, t) = g_1(t) \quad t \in [0, 1] \\
		u(5, t) = g_2(t) \quad t \in [0, 1] \\
		u(x, 0) = h(x) \quad x \in [0, 5] 
	\end{cases}
\end{equation}

我们取扩散系数$\nu=0.01$，对流速度$v=0.01$，真解$u(x, t) = e^{-t}\sin(\pi(x + t + x\cdot t))$，则$f(x,t), g_1(t), g_2(t), h(x)$可相应的计算出来。

对方程使用改进的ST-RFM进行数值求解，超参数的设置如下：

\begin{itemize}
	\item 空间维度的划分数量为$N_x = 5$；
	\item 时间维度的划分数量为$N_t = 2$；
	\item 每个子区域的空间维度的配点数量为$Q_x = 10$；
	\item 每个子区域的时间维度的配点数量为$Q_t = 10$；
	\item 每个子区域的特征函数的数量为$J_n=150$；
\end{itemize}

所以每个子区域的配点数量为$Q_n = Q_x \times Q_t = 100$，求解结果见

\bibliography{references}
\end{document}