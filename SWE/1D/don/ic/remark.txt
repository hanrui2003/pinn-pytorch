swe_1d_don_01.py  每种数据集设置不同的batch_size，但是相对于各自数据集的大小是等比的。比如都是百分之十。
swe_1d_don_02.py  batch_size设置为统一的，比如10000
swe_1d_don_01_1_3e-5.pt  区间长度1，mse<3e-5
swe_1d_don_01_0.5_e-5.pt 区间长度0.5，mse<e-5

swe_1d_don_03.py  初值参数（分支网的输入），选取[0,0.1]上的十个点为高斯函数的均值mu，生成初值；
物理信息点设为10w。边值条件是速度为0. batch_size 统一10000，训练的区间为[0,1]*[0,1]
swe_1d_don_03_e-5.pt mse<e-5 Elapsed time:  0:25:06.312997 测试效果不太好
swe_1d_don_03_5e-6.pt  Elapsed time:  0:47:37.368738  波整体相似，但贴合的不好
swe_1d_don_03_e-6.pt batch : 94300 lr : 0.001  Elapsed time:  3:49:19.280662
前0.4s，相对误差在e-3,后面开始不太好 (注：如果吧数值解法的网格改为201*201,那么相对误差基本在e-3)

swe_1d_don_04.py
相对于 swe_1d_don_03.py，更改训练区间为[0,1]*[0,0.5]，相应的物理点设为5000.其他不变
swe_1d_don_04_5e-6.pt batch : 14743 lr : 0.0005 loss : 4.920952960674185e-06 Elapsed time:  0:25:25.527434
swe_1d_don_04_e-6.pt  batch : 47022 lr : 0.00025 loss : 9.870293524727458e-07 Elapsed time:  1:21:03.859801

swe_1d_don_tbc_ic_03.py
逻辑和数据几乎和swe_1d_don_03.py 一致，区别即使把主干网和分支网的输入拼接，然后使用一个全连接网络。层数相同，单元数和分支网（或主干网相同）
训练显存占用 773MiB
swe_1d_don_tbc_ic_03_e-5.pt  Elapsed time:  0:21:13.706821 测试效果不太好，不过比swe_1d_don_03_e-5.pt 略好。