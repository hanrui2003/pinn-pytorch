# autograd
pytorch里面的求导有个坑，需要特别注意，它的求导不适用于向量对向量求导。它只能标量对标量或向量求导。

不管是求一阶导、二阶导、还是任意阶导数，如果使用autograd.grad求
(y_1,y_2,...,y_n)对(x_1,x_2,...,x_m)的导数，则pytorch的求导逻辑是

y_1对(x_1,x_2,...,x_m)，得到(dy_1/dx_1,dy_1/dx_2,...,dy_1/dx_m)

y_2对(x_1,x_2,...,x_m)，得到(dy_2/dx_1,dy_2/dx_2,...,dy_2/dx_m)

...

y_n对(x_1,x_2,...,x_m)，得到(dy_n/dx_1,dy_n/dx_2,...,dy_n/dx_m)

最后将n个结果做向量的加法，所以得到的结果往往不是想要的。

所以记住，不管求几阶导，应变量一定要标量，这里说的标量指的是(batch_size,1)的输入向量，第二个维度一定是1.