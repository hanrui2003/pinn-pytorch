lorenz63_data_chaos.py
使用RK方法，[1.508870, -1.531271, 25.46091]为初值，生成混沌的数值解，时间区间[0,14.995]，保存到文件 lorenz63_chaos.npy

lorenz63_data_non_chaos.py
使用RK方法，[-4., 7., 15]为初值，生成非混沌的数值解，时间区间[0,14.995]，
保存到文件 lorenz63_non_chaos.npy

lorenz63_don_01.py
考虑的是非混沌情形（rho=15），用的原始的DeepONet架构，训练的时间区间[0,0.5]，
使用高斯核函数进行密度估计，得到样本密度函数，然后随机生成初值点10000个，作为训练数据。
物理配点等距取100个，
lorenz63_don_01_05_gzz_01.pt 是训练出的最好的模型
在测试模块lorenz63_don_01_test.py 中使用种子数 np.random.seed(123)，可以得到较好的结果；

lorenz63_don_02.py
考虑的是混沌情形（rho=28），直接采用数值解作为训练样本。用的原始的DeepONet架构，代码复用，训练的时间区间分别是[0,0.1]和[0,0.2]，
物理配点等距取40个，
lorenz63_don_02_01_-1_gzz_01.pt
后缀中的01应该表示训练区间长度0.1，-1表示训练条件是loss达到10^{-1}，gzz表示工作站的训练结果。
其他pt文件亦然。
当训练区间为[0,0.2]的时候，整体来说不理想，一来是 训练的数据不是由密度函数随机采样，二来测试的误差较大。
当训练期间为[0,0.1]的时候，效果还行，lorenz63_don_02_test2.py + lorenz63_don_02_01_-1_gzz_non_random.pt测试
测试区间[0,2]的时候相对误差到了1%的量级，挺好，但是随着测试区间加大到[0,5], 相对误差就有18%，比较大。说明训练样本还没能表现整体分布。
而且训练的数据不是由密度函数随机采样
（注：lorenz63_don_02_01_-1_gzz_non_random.pt 和 lorenz63_don_02_01_-2_gzz_non_random.pt 根据测试，应该是同一个模型，不记得当初为什么命名两个文件）
重大发现！！！！，之前的思路是有问题的，之前总是把算子网络输出的最后一个值作为新的初值迭代求解，其实这是错的 ，
因为是混沌的，但凡有点小误差，后面的差异就会非常大。
所以应该在一些时间点引入观测作为新的初值，代码中就用一些离散时间点的数值解作为观测，
这样误差瞬间变的很小，相对误差控制在千分位，完全符合期望。np.random.seed(1234)的结果可写到论文。


lorenz63_don_03.py
考虑的是混沌情形（rho=28），代码复用，训练的时间区间分别是[0,0.1]和[0,0.2]，
物理配点等距取40个，最后一层激活函数用sin，
使用高斯核函数进行密度估计，得到样本密度函数，然后随机生成初值点10000个，作为训练数据。
测试效果，都不好。PASS

