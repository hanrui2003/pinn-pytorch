\documentclass{article}
\usepackage[a4paper,innermargin=1.2in,outermargin=1.2in,
bottom=1.5in,marginparwidth=1in,marginparsep
=3mm]{geometry}
\usepackage{amsmath,amsthm,amssymb,enumerate}
\usepackage{ctex}
\usepackage{natbib}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{float}
\usepackage{subfigure}
\usepackage{color}
\usepackage{array}
\title{神经网络解Lorenz63方程}
\author{MG21210021李庆春}
%\date{2022.8.20}
\linespread{1.25}
\bibliographystyle{plain}

\usepackage{listings}
\lstset{
	columns=fixed,       
	numbers=left,                                        % 在左侧显示行号
	numberstyle=\tiny\color{black},                       % 设定行号格式
	frame=none,                                          % 不显示背景边框
	backgroundcolor=\color[RGB]{245,245,244},            % 设定背景颜色
	keywordstyle=\color[RGB]{40,40,255},                 % 设定关键字颜色
	numberstyle=\footnotesize\color{black},           
	commentstyle=\it\color[RGB]{0,96,96},                % 设置代码注释的格式
	stringstyle=\rmfamily\slshape\color[RGB]{128,0,0},   % 设置字符串格式
	showstringspaces=false,                              % 不显示字符串中的空格
	language=python,                                        % 设置语言
}


\begin{document}
	
	\maketitle
	
	\section{Lorenz63方程简述}
	\begin{equation}
		\begin{cases}
			\frac{dx}{dt} = \sigma(y -x) \\
			\frac{dy}{dt} = x(\rho - z) - y \\
			\frac{dz}{dt} = xy - \beta z
		\end{cases}
	\end{equation}
	
	\section{非混沌情形}
	$\rho = 15 , \sigma = 10 , \beta = \frac{8}{3}$ 初值：[-8., 7., 27.]，RK方法的数值解如图\ref{lorenz63-rk}：
	\begin{figure}[h]
		\centering
		\subfigure[lorenz63 二维视图.]{
			\includegraphics[width=0.45\linewidth]{lorenz63-rk-non-chaos-01}
		}
		\subfigure[lorenz63 三维视图.]{
			\includegraphics[width=0.45\linewidth]{lorenz63-rk-non-chaos-02}
		}
		\caption{lorenz63-RK}
		\label{lorenz63-rk}
	\end{figure}
	
	\subsection{不带观测的神经网络}
	损失函数使用均方误差（MSE），网络结构：[1, 32, 32, 3]，初值：[-8, 7, 27]，迭代轮数：500000，对训练数据进行正规化，关键代码如下：\\
	\begin{lstlisting}
		scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 
		min_lr=1e-7, mode='min',factor=0.5,patience=50000,verbose=True)
		self.loss_func = nn.MSELoss(reduction='mean')
		
		layers = [1, 32, 32, 3]
		
		total_points = 300
		x_lb = torch.tensor(0.)
		x_ub = torch.tensor(1.24)
		
		rho = torch.tensor(15.0)
		sigma = torch.tensor(10.0)
		beta = torch.tensor(8.0 / 3.0)
		
		x_train_bc = torch.tensor([[0.]])
		y_train_bc = torch.tensor([[-8., 7., 27.]])
		
		epochs = 500000
	\end{lstlisting} 
	通过调整区间长度发现，最多可训练出[0, 1.24]的结果，误差在$10^{-4}$量级，如图\ref{lorenz63-dnn-non-chaos-[1,1.24]}：
	\begin{figure}[h]
		\centering
		\subfigure[lorenz63 二维视图.]{
			\includegraphics[width=0.45\linewidth]{lorenz63-dnn-non-chaos-01}
		}
		\subfigure[lorenz63 三维视图.]{
			\includegraphics[width=0.45\linewidth]{lorenz63-dnn-non-chaos-02}
		}
		\caption{lorenz63-DNN}
		\label{lorenz63-dnn-non-chaos-[1,1.24]}
	\end{figure}
	
	如果区间较大，则训练不出想要的结果，神经网络会趋于零，如图\ref{lorenz63-dnn-non-chaos-failed-01}展示的是[0, 3]的训练结果：
	\begin{figure}[h]
		\centering
		\subfigure[lorenz63 二维视图.]{
			\includegraphics[width=0.45\linewidth]{lorenz63-dnn-non-chaos-03}
		}
		\subfigure[lorenz63 三维视图.]{
			\includegraphics[width=0.45\linewidth]{lorenz63-dnn-non-chaos-04}
		}
		\caption{lorenz63-DNN}
		\label{lorenz63-dnn-non-chaos-failed-01}
	\end{figure}
	
	\section{混沌情形}
	$\rho = 28 , \sigma = 10 , \beta = \frac{8}{3}$ ，初值[-4., 7., 15.]，RK方法的数值解如下图：（暂时没贴）
	\subsection{整数阶Lorenz方程}
	接下来用神经网络训练，离散步长h=0.005，区间[0, 1.5]，观测值如下：\\
	\begin{tabular}{| p{1.5cm}<{\centering} || p{1.5cm}<{\centering} | p{1.5cm}<{\centering} | p{1.5cm}<{\centering} | } \hline
		t & x & y & z  \\ \hline \hline
		0.0000 & -4.0000 &   7.0000 &  15.0000 \\ \hline
		0.5000 & 5.6789 &  -2.6593 &  33.0093 \\ \hline
		1.0000 & -12.0143 & -17.4640 &  24.3640 \\ \hline
		1.500 & -1.1838 &  -1.6864 &  15.08598 \\ \hline
	\end{tabular}
	\subsection{分数阶Lorenz方程}
	\subsubsection{Caputo导数及数值逼近}
	Caputo分数阶导数的定义：
	\begin{equation}
		_aD_t^\alpha f(t) = \frac{1}{\Gamma(n - \alpha)}\int_a^t (t - \tau)^{n-\alpha-1} f^{(n)}(\tau) d\tau  \qquad (n-1 \leq \alpha < n).
	\end{equation}
	当a=0时，简记为$D^\alpha$。\\
	分数阶微积分的差商逼近格式可以写成如下统一的形式：
	\begin{equation}
		D^\alpha f(t_n) \approx h^{-\alpha}\sum_{k=0}^{N}c_{n,k}f_k
	\end{equation}
	L1算法（即$0\leq\alpha<1$ ，上式中N=n）：
	\begin{equation}
		c_{n,k} = \frac{1}{\Gamma(2-\alpha)}
		\begin{cases}
			-c_{n-1},     & k = 0; \\
			c_{n-k} - c_{n-k-1},  & 1\leq k \leq n-1; \\
			1,   & k=n; \\
			0,   & else.
		\end{cases}
	\end{equation}
	\begin{equation}
		(D_t^\alpha f(t_n))_{L1} = \frac{1}{\Gamma(2 - \alpha)h^\alpha}\sum_{k=0}^{n-1}c_k(f_{n-k} - f_{n-k-1})
	\end{equation}
	其中，$c_l = (l+1)^{1-\alpha} - l^{1-\alpha}$，fPINN论文中另一种等价表述如下：
	
	\begin{equation}
		\begin{aligned}
			&\frac{\partial^\gamma \tilde{u} (\boldsymbol{x}, t)}{\partial t^\gamma} 
			\approx 
			\frac{1}{\Gamma (2 - \gamma)(\Delta t)^\gamma} \\
			&\bigg \{
			-c_{\lceil \lambda t \rceil - 1} \tilde{u} (\boldsymbol{x}, 0) + 
			c_0 \tilde{u} (\boldsymbol{x}, t) + 
			\sum_{k=1}^{\lceil \lambda t \rceil - 1} (c_{\lceil \lambda t \rceil - k} - c_{\lceil \lambda t \rceil - k - 1}) \tilde{u} (\boldsymbol{x}, k\Delta t)
			\bigg \}, \\
			&0 < \gamma < 1
		\end{aligned}
	\end{equation}
	时间步长 $\Delta t = t/\lceil \lambda t \rceil \approx 1/\lambda$，$\lceil \cdot \rceil $ 是向上取整函数，表示离散化后的区间数，相当于L1算法公式中的n；常量因子$\lambda$决定着步长，如$\lambda = 200$，则意味着把单位1长度划分为200个小区间。由于计算机在运算时，存在舍入误差，所以$c_0 \tilde{u} (\boldsymbol{x}, n\Delta t) $写成$c_0 \tilde{u} (\boldsymbol{x}, t) $。\\
	{\color{red} (这里有疑问，t=0的导数是不是没法计算) } \\
	所以，可先对每个训练点计算出其时间划分，然后拼成一个大向量。 \\
	如果不引入观测，训练结果的趋势是趋于0，不符合预期；
	
	\subsubsection{分数阶常微分方程数值解法}
	（直接法）考虑齐次初值条件的分数阶微分方程：
	\begin{equation}
		\begin{cases}
			\frac{\partial^\alpha y(t)}{\partial t^\alpha} = f(t,y(t)), \quad t\in [0,T], \\
			y^{(k)}(0) = 0, \quad k = 0,1,\cdots , m-1.
		\end{cases}
	\end{equation}
	则直接应用分数阶导数的一般差商逼近公式得：
	\begin{equation}
		h^{-\alpha}\sum_{k=0}^{N}c_{n,k}y_k = f(t_n,y_n), \quad n = 0,1,\cdots,[t/h]
	\end{equation}
	上式左边为y在t处的$\alpha$阶导数，注意此时将[0,t]分成了n份，每份长度为h，t对应$t_n$，$y(t_n)$对应$y_n$。则上面方程组可以按下面的方式逐点计算：
	\begin{equation}
		y_N = \frac{h^\alpha}{c_{n,N}}f(t_n,y_n)  - \frac{1}{c_{n,N}}\sum_{k=1}^{N-1}c_{n,k}y_k , \quad n =1,\cdots,[T/h]
	\end{equation}
	其中，N=n(对应到G1算法、D算法、L1算法、线性多步法)或N=n+1（对应到G2算法、L2算法）。上式中，由于$y_0=0$，所以求和项从k=1开始，对于一般的L1算法，可写成如下形式：
	\begin{equation}
		\begin{aligned}
			y_n &= \frac{h^\alpha}{c_{n,n}}f(t_n,y_n)  - \frac{1}{c_{n,n}}\sum_{k=0}^{n-1}c_{n,k}y_k , \quad n =1,\cdots,[T/h] \\
			&= \Gamma(2 - \alpha)h^\alpha f(t_n,y_n) - \sum_{k=1}^{n-1} (c_{n-k}-c_{n-k-1})y_k + c_{n-1}y_0 \\
			&= \Gamma(2 - \alpha)h^\alpha f(t_n,y_n) - \sum_{k=1}^{n-1}[(n-k+1)^{1-\alpha} -2(n-k)^{1-\alpha} + (n-k-1)^{1-\alpha}]y_k + \\
			&\qquad [n^{1-\alpha} - (n-1)^{1-\alpha}]y_0
		\end{aligned}
	\end{equation}
	
	\subsubsection{简单的例子}
	已知正弦、余弦函数的整数阶微分表达式分别为
	\begin{equation}
		\frac{d^k}{dt^k}[\sin at] = a^k \sin(at+\frac{k\pi}{2}), \quad \frac{d^k}{dt^k}[\cos at] = a^k \cos(at+\frac{k\pi}{2})
	\end{equation}
	由Cauchy积分公式可以证明，对于分数阶的微分方程来说，当k为分数时，上述公式仍然成立。所以考虑如下分数阶微分方程：
	\begin{equation}
		\begin{cases}
			\frac{d^\alpha u}{dt^\alpha} = \sin(t+\frac{\alpha\pi}{2}), \qquad 0<\alpha<1 \\
			u(0) = 0
		\end{cases}
	\end{equation}
	（这里暂时不知道解析解，所以作废）
	
	对于$f(t)=t^\lambda, \lambda>-1$，n为大于 $\alpha$的最小整数，则其$\alpha$阶Caputo导数为：
	\begin{equation}
		\begin{aligned}
			D^\alpha t^\lambda &= \frac{1}{\Gamma(n-\alpha)}\int_0^t (t-\tau)^{n-\alpha-1}\frac{d^n \tau^\lambda}{d\tau^n}d\tau \\
			&= \frac{1}{\Gamma(n-\alpha)}\int_0^t (t-\tau)^{n-\alpha-1} \lambda(\lambda-1)\cdots(\lambda-n+1)t^{\lambda-n} d\tau \\
			&= \frac{\Gamma(\lambda+1)}{\Gamma(n-\alpha)\Gamma(\lambda-n+1)}\int_0^t (t-\tau)^{n-\alpha-1} t^{\lambda-n} d\tau \\
			&= \frac{\Gamma(\lambda+1)}{\Gamma(n-\alpha)\Gamma(\lambda-n+1)} t^{\lambda-\alpha}\int_0^1 (1-\tau)^{n-\alpha-1} t^{\lambda-n} d\tau \quad (substitution) \\
			&= \frac{\Gamma(\lambda+1)}{\Gamma(n-\alpha)\Gamma(\lambda-n+1)} t^{\lambda-\alpha} B(n-\alpha, \lambda-n+1) \\
			&= \frac{\Gamma(\lambda+1)}{\Gamma(n-\alpha)\Gamma(\lambda-n+1)} t^{\lambda-\alpha} \frac{\Gamma(n-\alpha)\Gamma(\lambda-n+1)}{\Gamma(\lambda-\alpha+1)}  \\
			&=\frac{\Gamma(\lambda+1)}{\Gamma(\lambda-\alpha+1)} t^{\lambda-\alpha}
		\end{aligned}
	\end{equation}
	这和R-L导数一致，这是由于$D^n t^\lambda$在区间$[0, +\infty)$ 上连续，且$D^kf(0)=0, k=0,1,\cdots,n-1$，所以$D^n$和$D^{-\nu}$可交换。\\
	例子：考虑$y=t^3$，则对应的分数阶微分方程为：
	\begin{equation}
		\begin{cases}
			D^\alpha t^3 = \frac{6}{\Gamma(4-\alpha)} t^{3-\alpha} \\
			y(0) = 0
		\end{cases}
	\end{equation}
	写成隐式的：
	\begin{equation}
		\begin{cases}
			D^\alpha u = \frac{6u}{\Gamma(4-\alpha)} t^{-\alpha} \\
			u(0) = 0.000001
		\end{cases}
	\end{equation}
	数值计算结果如图\ref{ode-fdm}：{\color{red} (这里有疑问，时间是不是一定要从0开始，变成时发现如果不是0作为离散的初始点，则得不到结果) }
	\begin{figure}[h]
		\centering
		\subfigure[显示]{
			\includegraphics[width=0.45\linewidth]{frac-ode-fdm-explicit-01}
		}
		\subfigure[隐式]{
			\includegraphics[width=0.45\linewidth]{frac-ode-fdm-implicit-01}
		}
		\caption{ODE-FDM}
		\label{ode-fdm}
	\end{figure}
	
	\subsubsection{分数阶Lorenz63方程}
	\begin{equation}
		\begin{cases}
			\frac{d^\alpha x}{dt^\alpha} = \sigma(y -x) \\
			\frac{d^\alpha y}{dt^\alpha} = x(\rho - z) - y \\
			\frac{d^\alpha z}{dt^\alpha} = xy - \beta z
		\end{cases}
	\end{equation}
	取$\alpha = 0.99$，离散步长h=0.005，区间[0, 3]，隐式收敛阈值$\epsilon = 0.001$，初值[-4., 7., 15.]，数值计算结果如图\ref{florenz63-FDM}：
	\begin{figure}[h]
		\centering
		\subfigure[分数阶lorenz63-2D]{
			\includegraphics[width=0.45\linewidth]{florenz63-fdm-2d-01}
		}
		\subfigure[分数阶lorenz63-3D]{
			\includegraphics[width=0.45\linewidth]{florenz63-fdm-3d-01}
		}
		\caption{frac-lorenz63-FDM}
		\label{florenz63-FDM}
	\end{figure}
	
	接下来用神经网络训练，损失函数使用MSE，先用参数$\alpha = 0.99$，离散步长h=0.005，区间[0, 1.5]，观测值如下：\\
	\begin{tabular}{| p{1.3cm}<{\centering} || p{1.3cm}<{\centering} | p{1.3cm}<{\centering} | p{1.3cm}<{\centering} | } \hline
		t & x & y & z  \\ \hline \hline
		0.0050 & -3.4659 &  6.7226 & 14.6728 \\ \hline
		0.2500 & 10.4394 & 17.7745 & 17.9601 \\ \hline
		0.5000 & 5.6236 & -0.6432 & 31.2168 \\ \hline
		0.7500 & 0.7111 &  0.8086 & 15.8068 \\ \hline
		1.0000 & 4.2548 &  7.9222 &  9.8927 \\ \hline
		1.2500 & 13.9364 &  7.5091 & 39.7435 \\ \hline
		1.5000 & 0.5546 & -0.6693 & 20.8581 \\ \hline
	\end{tabular} \\
	训练结果如图\ref{florenz63-dnn-01}，符合预期：
	\begin{figure}[h]
		\centering
		\subfigure[分数阶lorenz63-2D]{
			\includegraphics[width=0.45\linewidth]{florenz63-dnn-01-2d}
		}
		\subfigure[分数阶lorenz63-3D]{
			\includegraphics[width=0.45\linewidth]{florenz63-dnn-01-3d}
		}
		\caption{frac-lorenz63-FDM}
		\label{florenz63-dnn-01}
	\end{figure}

继续训练[1.5, 3]，观测数据如下，注意此时t=0对应的是t=1.5：\\
\begin{tabular}{| p{1.6cm}<{\centering} || p{1.6cm}<{\centering} | p{1.6cm}<{\centering} | p{1.6cm}<{\centering} | } \hline
	t & x & y & z  \\ \hline \hline
	0.0050 & 0.4947 &  -0.6466 &  20.5689 \\ \hline
	0.2500 & -0.7559 &  -1.3882 &  10.706 \\ \hline
	0.5000 & -7.8296 & -14.6883 &  11.2953 \\ \hline
	0.7500 & -7.6905 &   1.1865 &  34.9326 \\ \hline
	1.0000 & 1.0532 &   2.0665 &  17.3576 \\ \hline
	1.2500 & 7.2898 &  12.7107 &  14.2827 \\ \hline
	1.5000 & 9.4016 &   2.1509 &  35.2638 \\ \hline
\end{tabular} \\
训练结果如图\ref{florenz63-dnn-02}，除个别点尖锐突变，根据几次训练，突变点在[0.4,0.8]之间，有时是个小尖角，其他部分符合预期：
\begin{figure}[h]
	\centering
	\subfigure[分数阶lorenz63-2D]{
		\includegraphics[width=0.45\linewidth]{florenz63-dnn-02-2d}
	}
	\subfigure[分数阶lorenz63-3D]{
		\includegraphics[width=0.45\linewidth]{florenz63-dnn-02-3d}
	}
	\caption{frac-lorenz63-FDM}
	\label{florenz63-dnn-02}
\end{figure}
所以继续对[0.4,0.8]增加一些观测值，如下：\\
\begin{tabular}{| p{1.6cm}<{\centering} || p{1.6cm}<{\centering} | p{1.6cm}<{\centering} | p{1.6cm}<{\centering} | } \hline
	t & x & y & z  \\ \hline \hline
	0.4000 & -3.0048 &  -5.7771 &   8.0258 \\ \hline
	0.6000 & -15.5885 & -21.0788 &  30.5955 \\ \hline
	0.7000 & -12.4916 &  -3.5173 &  39.8045 \\ \hline
	0.8000 & -3.9029 &   2.2364 &  29.9654 \\ \hline
\end{tabular} \\
得到的训练结果如图\ref{florenz63-dnn-03}：
\begin{figure}[h]
	\centering
	\subfigure[分数阶lorenz63-2D]{
		\includegraphics[width=0.45\linewidth]{florenz63-dnn-03-2d}
	}
	\subfigure[分数阶lorenz63-3D]{
		\includegraphics[width=0.45\linewidth]{florenz63-dnn-03-3d}
	}
	\caption{frac-lorenz63-FDM}
	\label{florenz63-dnn-03}
\end{figure}
继续增加观测值，变为每0.1的长度，引入一个观测值，训练结果如图\ref{florenz63-dnn-04}：
\begin{figure}[h]
	\centering
	\subfigure[分数阶lorenz63-2D]{
		\includegraphics[width=0.45\linewidth]{florenz63-dnn-04-2d}
	}
	\subfigure[分数阶lorenz63-3D]{
		\includegraphics[width=0.45\linewidth]{florenz63-dnn-04-3d}
	}
	\caption{frac-lorenz63-FDM}
	\label{florenz63-dnn-04}
\end{figure}

\section{Burgers方程}
\begin{equation}
	\begin{cases}
		\frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} = \nu \frac{\partial^2 u}{\partial x^2}  \\
		x \in [-1, 1] \\
		t \in [0, 1]
	\end{cases}
\end{equation}

\section{TODO}
lorenz63时间长的时候，趋于0，什么原因？\\
pinn解PDE的论文，多看看，多总结，作为自己毕设素材 \\
GL算法实现 \\
神经网络解burgers \\

lorenz63-dnn-obs-01  [0,3] 训练 每个0.02 一个观测值，观测值由RK法计算，初值为：[1.508870, -1.531271, 25.46091] ，训练不收敛

lorenz63-dnn-obs-02 [0,1.5]
lorenz63-dnn-obs-03 [0,3] 观测值为iobsdisturb.txt , 导师发是数据；

lorenz63-dnn-12 [0,3]  尝试分段mse，意图为规避训练区间加长，后续趋于0 ，失败，仍然趋于0。换个思路， 把三个导数的平方和相加，取导数加入损失函数。失败

lorenz63-dnn-13 使用L1Smooth + 无穷范数，失败

\section{DeepONet}
\subsection{理论背景}
一般的函数：
\begin{equation}
	z=f_1(x)=sin(x) \in[-1, 1]
\end{equation}
算子：
\begin{equation}
	 G(f_1(x))=f_2(x)
\end{equation}
比如微分算子：
\begin{equation}
	f_2=\frac{df_1(x)}{d x}=\frac{d}{d x}sin(x)=cos(x)
\end{equation}
如果我们的PDE里面有参数（比如形状、初边值、扩散系数等），这里统一用u(x)表示，我们是允许它变化的。这个时候，每个u对应一个真解s；参数化的PDE可写成如下形式：
\begin{equation}
	\mathcal{N}(u,s)=0
\end{equation}
PDE的解算子写成（此时，我们可以把PDE的一般解表示为算子G）：
\begin{equation}
	G(u)=s
\end{equation}
此时，u，s均为函数，并且满足：
\begin{equation}
	G(u)(y)=s(y) \in \mathbf{R}
\end{equation}
算子的通用逼近定理：\\
$\forall \epsilon >0$, 存在正整数 $n,p,m$, 常量 $c_i^k,W_{bij}^k,b_{bij}^k,W_{tk},b_{tk}$ 使得:
\begin{equation}
	\left|G(u)(y)-\sum_{k=1}^{p} \underset{Branch}{\underbrace{ \sum_{i=1}^{n}c_i^k\sigma\left(\sum_{j=1}^{m}W_{bij}^{k}u(x_j)+b_{bi}^k\right)}}. \underset{Trunk}{\underbrace{\sigma(W_{tk}.y+b_{tk})}}\right|<\epsilon
\end{equation}
神经网络：
\begin{equation}
	NN(X)=W_n\sigma_{n-1}(W_{n-1}\sigma_{n-2}(...(W_2\sigma_1(W_1X+b_1)+b_2)+..)+b_{n-1})+b_n
\end{equation}
所以说，我们可以用两个神经网络去逼近算子，其中分支网络为：
\begin{equation}
	NN_b(u(\textbf{x}))=b(u(\textbf{x}))=\textbf{c}.\sigma\left(W_{b}u(\textbf{x})+\textbf{b}_{b}\right)
\end{equation}
主干网络：
\begin{equation}
	NN_t(\textbf{y})=t(\textbf{y})=\sigma(W_{t}.\textbf{y}+\textbf{b}_{t})
\end{equation}
DeepONet（注意：$\theta$ 是网络参数，即weight和bias）
\begin{equation}
	G_\theta(u)(y)=\sum_{k=1}^q\underset{Branch}{\underbrace{b_k\left(u(x_1),u(x_2),...,u(x_m)\right)}}.\underset{Trunk}{\underbrace{t_k(\textbf{y})}}
\end{equation}

\begin{figure}[h]
	\centering
	\subfigure[DeepONet架构.]{
		\includegraphics[width=0.95\linewidth]{deeponet-architecture}
	}
	\caption{DeepONet架构}
	\label{DeepONet架构}
\end{figure}

神经网络的输出应该逼近真解算子
\begin{equation}
	G_\theta(u)(y)\approx G(u)(y)
\end{equation}
我们把这个约束加入损失函数
\begin{equation}
	\mathcal{L}_{Operator}(\theta)=\frac{1}{NP}\sum_{i=1}^N\sum_{j=1}^P\left|G_{\theta}(u^{(i)})y_j^{(i)}-G(u^{(i)})y_j^{(i)}\right|^2
\end{equation}
\begin{equation}
	\mathcal{L}_{Operator}(\theta)=\frac{1}{NP}\sum_{i=1}^N\sum_{j=1}^P\left|\sum_{k=1}^q{b_k\left(u^{(i)}(x_1),u^{(i)}(x_2),...,u^{(i)}(x_m)\right)}.t_k(y_j^{(i)})-G(u^{(i)})y_j^{(i)}\right|^2
\end{equation}
Physics-informed DeepONets
\begin{equation}
	\mathcal{L}_{Physics}(\theta)=\frac{1}{NQm}\sum_{i=1}^{N}\sum_{j=1}^{Q}\sum_{k=1}^{m}\left|\mathcal{N}(u^{(i)}(x_k),G_{\theta}(u^{(i)})(y_j^{(i)})\right|^2
\end{equation}
\begin{equation}
	\mathcal{L}(\theta)=\mathcal{L}_{Operator}(\theta)+\mathcal{L}_{Physics}(\theta)
\end{equation}

\subsection{示例：反应扩散方程}
\begin{equation}
	\frac{\partial s}{\partial t}=D\frac{\partial^2 s}{\partial x^2}+ks^2+u(x) \qquad (x,t) \in (0,1] \times(0,1]
\end{equation}
其中，扩散系数D=0.01，反应速率k=0.01；u(x)是源项，初边值条件为0；（可以看出PI-DON不需要太多的训练数据）\\
对于任意给的$u^{(i)}$，我们有：
\begin{equation}
	u^{(i)}=\frac{\partial s^{(i)}}{\partial t}-D\frac{\partial^2 s^{(i)}}{\partial x^2}-k[s^{(i)}]^2
\end{equation}
理想情况下，我们的DeepONet拟合的算子满足：
\begin{equation}
	G_{\theta}(u^{(i)})(x,t)\approx G(u^{(i)})(x,t)= s^{(i)}(x,t)
\end{equation}
\begin{equation}
	u^{(i)}\approx \frac{\partial G_{\theta}(u^{(i)})(x,t)}{\partial t}-D\frac{\partial^2 G_{\theta}(u^{(i)})(x,t)}{\partial x^2}-k[G_{\theta}(u^{(i)})(x,t)]^2
\end{equation}
我们记右端项
\begin{equation}
	R_{\theta}^{(i)}(x,t)=\frac{\partial G_{\theta}(u^{(i)})(x,t)}{\partial t}-D\frac{\partial^2 G_{\theta}(u^{(i)})(x,t)}{\partial x^2}-k[G_{\theta}(u^{(i)})(x,t)]^2
\end{equation}
则物理模型损失：
\begin{equation}
	\mathcal{L}_{Physics}(\theta)=\frac{1}{NQ}\sum_{i=1}^{N}\sum_{j=1}^{Q}\left|R_{\theta}^{(i)}(x_{r,j}^{(i)},t_{r,j}^{(i)})-u^{(i)}(x_{r,j}^{(i)})\right|^2
\end{equation}
其中，$(x_{r,j},t_{r,j})$是配置点；\\
另一方面，用零初边值条件计算算子损失：
\begin{equation}
	\mathcal{L}_{Operator}(\theta)=\frac{1}{NQ}\sum_{i=1}^{N}\sum_{j=1}^{P}\left|G_{\theta}(u^{(i)})(x_{u,j}^{(i)},t_{u,j}^{(i)})- G(u^{(i)})(x_{u,j}^{(i)},t_{u,j}^{(i)}))\right|^2
\end{equation}
其中，$(x_{u,j},t_{u,j})$是初边值条件上的点；因为这里是零初边值，所以
\begin{equation}
	G(u^{(i)})(x_{u,j}^{(i)},t_{u,j}^{(i)})=0
\end{equation}
\begin{equation}
	\mathcal{L}_{Operator}(\theta)=\frac{1}{NQ}\sum_{i=1}^{N}\sum_{j=1}^{P}\left|G_{\theta}(u^{(i)})(x_{u,j}^{(i)},t_{u,j}^{(i)})))\right|^2
\end{equation}
最终，损失函数为：
\begin{equation}
	\mathcal{L}(\theta)=\mathcal{L}_{Ooperator}(\theta)+\mathcal{L}_{Physics}(\theta)
\end{equation}

\subsubsection{训练算法}
\begin{enumerate}
\item 选择N个源项，也就是输入函数，
	\begin{equation}
		\{u^{(i)}(\textbf{x})\}_{i=1}^{N}={[u^{(1)}(\textbf{x}),u^{(2)}(\textbf{x}),...,u^{(N)}(\textbf{x})]}
	\end{equation}
\item 在m个点（即输入张量）评估N个输入函数
	\begin{equation}
		\begin{bmatrix}
			u^{(1)}(x_1) & u^{(1)}(x_2)& ... & u^{(1)}(x_m) \\
			u^{(2)}(x_1) & u^{(2)}(x_2)& ... & u^{(2)}(x_m) \\
			\vdots & \vdots & \ddots & \vdots \\
			u^{(N)}(x_1) & u^{(N)}(x_2) & ... & u^{(N)}(x_m)
		\end{bmatrix}
	\end{equation}
\item 将其送入分支网络，得到
	\begin{equation}
		b_k
		\begin{bmatrix}
			u^{(1)}(x_1) & u^{(1)}(x_2)& ... & u^{(1)}(x_m) \\
			u^{(2)}(x_1) & u^{(2)}(x_2)& ... & u^{(2)}(x_m) \\
			\vdots & \vdots & \ddots & \vdots \\
			u^{(N)}(x_1) & u^{(N)}(x_2) & ... & u^{(N)}(x_m)
		\end{bmatrix}
	\end{equation}
\item 从初边值选择P个点（即输出张量）
	\begin{equation}
		\textbf{y}_u=y_{u1},y_{u2},...,y_{uP}
	\end{equation}
\item 将其送入主干网络，得到
	\begin{equation}
		t_k(y_{u1},y_{u2},...,y_{uP})
	\end{equation}
\item 将二者输出做点积，得到近似的算子
	\begin{equation}
		G_\theta(u)(\textbf{y})=\sum_{k=1}^q\underset{Branch}{\underbrace{b_k\left(u(x_1),u(x_2),...,u(x_m)\right)}}.\underset{Trunk}{\underbrace{t_k(\textbf{y})}}
	\end{equation}
\item 理想情况下，$G_\theta(u)(\textbf{y}_u)\approx G(u)(\textbf{y}_u)=0$ ，所以只需计算损失：
	\begin{equation}
		\mathcal{L}_{Operator}(\theta)=\frac{1}{NP}\sum_{i=1}^N\sum_{j=1}^P\left|G_{\theta}(u^{(i)})y_{uj}^{(i)}\right|^2
	\end{equation}
\item 从定义域中随机选Q个配置点
	\begin{equation}
		\textbf{y}_r=y_{r1},y_{r2},...,y_{rQ}
	\end{equation}
\item 使用物理信息（PDE），计算配置点的损失
	\begin{equation}
		\mathcal{L}_{Physics}(\theta)=\frac{1}{NQ}\sum_{i=1}^{N}\sum_{j=1}^{Q}\left|R_{\theta}^{(i)}(y_{r,j}^{(i)})-u^{(i)}(x_{r,j}^{(i)})\right|^2
	\end{equation}
\item 计算总损失
	\begin{equation}
		\mathcal{L}(\theta)=\mathcal{L}_{operator}(\theta)+\mathcal{L}_{Physics}(\theta)
	\end{equation}
\item 更新神经网络参数使损失最小化
\item 重复上述过程直至$G_\theta(u)(x,t)\approx G(u)(x,t)$
\end{enumerate}

\subsubsection{代码中涉及的知识}
$\textbf{高斯径向基函数}$，是一种高斯核函数，用于计算两个时刻的高斯变量的协方差，用RBF得到的是一个半正定矩阵，注意：向量和自身做RBF的运算，得到的不一定是单位阵。而且这个核函数体现不出负相关性，如果要体现负相关性，用其他的核函数；
\begin{equation}
	k(x,x') = \sigma^2 exp(-\frac{||x-x'||^2}{2l^2})
\end{equation}
$\textbf{Cholesky分解}$\\
Cholesky 分解是把一个对称正定的矩阵表示成一个下三角矩阵L和其转置的乘积的分解。它要求矩阵的所有特征值必须大于零，故分解的下三角的对角元也是大于零的。Cholesky分解法又称平方根法。Cholesky分解通常用于解决线性方程组或生成随机数。\\
jax.numpy.linalg.cholesky 方法返回的是下三角矩阵，称为Cholesky因子。\\
对于一个正定矩阵$A$，可以通过Cholesky分解将其转换为$A=LL^T$的形式，然后利用标准正态分布的随机数生成方法，即使用均值为0，方差为1的正态分布生成随机数，生成一个$n$维向量$z$，其中$n$是矩阵$A$的维度。\\
然后，通过矩阵向量乘法，即$y=Lz$，可以得到一个满足多元正态分布$N(0,A)$的$n$维向量$y$。\\
因此，通过Cholesky分解可以将生成满足多元正态分布的随机数的过程转化为简单的矩阵运算，这在许多机器学习和统计模型中都是非常有用的。\\
为什么使用Cholesky分解的时候，往往会对正定矩阵做一点扰动？\\
在数值计算中，对于一个给定的正定矩阵，它的Cholesky分解不是唯一的。这意味着，如果两次使用Cholesky分解来生成随机数，即使使用相同的输入矩阵，也有可能得到不同的结果。\\
这是由于在使用 Cholesky 分解时，由于分解过程中需要进行平方根运算，当某些元素非常小甚至为零时，平方根运算会出现不稳定的情况，从而导致分解失败或者得到错误的结果。增加一个较小的扰动可以使得这些元素不再为零，从而避免这种不稳定性。\\
如果输入矩阵有某些特定的结构或数字精度问题，这种不确定性可能会增加，甚至导致错误的结果。为了解决这个问题，可以对输入矩阵进行微小的扰动，以确保得到稳定的Cholesky分解结果。\\
一种常用的扰动方法是在对角线上添加一个较小的正数$\epsilon$，这样可以确保输入矩阵的条件数（矩阵奇异值的比值）不会太大，从而使得Cholesky分解结果更加稳定。\\

$\textbf{数值解法}$\\
代码中的函数，是一个求解一般的一维扩散反应方程的程序，主要用到了高斯过程插值和有限差分的方法。以下是对代码的注释和说明：\\
\begin{enumerate}
	\item 在程序开始时，定义了微分方程的各项系数和边界条件：
	\begin{equation}
		\begin{cases}
			u_t = (k(x)u_x)_x - (v(x)u)_x + g(u) + f(x) \\
			k(x) = 0.01 \\
			v(x) = 0 \\
			g(u) = 0.01u^2 \\
			f(x) = Gausian Process Stochastic Function
		\end{cases}
	\end{equation}
	\item 在生成随机函数的时候，使用了高斯过程的方法。先选取一些离散点，计算协方差矩阵，然后进行 Cholesky 分解，得到一个随机向量，再通过一维分段线性插值得到一个连续的函数。
	\item 利用有限差分离散微分方程，得到一个三对角矩阵，然后求解线性方程组得到下一时刻的解。
	\item 时间上采用了显式的欧拉方法。
	\item 整个求解过程是以矩阵形式实现的，使用了 JAX 库进行了自动微分和并行加速。
\end{enumerate}
二阶导的中心差分：
\begin{equation}
	\frac{d^2s}{dx^2} \approx \frac{s_{i-1} - 2s_i + s_{i+1}}{\Delta x^2}
\end{equation}
\begin{equation}
	\frac{d^2\mathbf{s}}{dx^2} \approx \frac{1}{\Delta x^2}
	\begin{bmatrix}
		-2 & 1 & 0 & \cdots & 0 & 0 \\
		1 & -2& 1 & \cdots & 0 & 0 \\
		0 & 1 & -2 & \cdots & 0 & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0 & 0 & \cdots & -2 & 1 \\
		0 & 0 & 0 & \cdots & 1 & -2 \\
	\end{bmatrix}
	\begin{bmatrix}
		s_1 \\
		s_2 \\
		s_3 \\
		\vdots \\
		s_{N-2} \\
		s_{N-1} \\
	\end{bmatrix}
	= \frac{1}{\Delta x^2} D2\cdot \mathbf{s}
\end{equation}
一阶导的中心差分：
\begin{equation}
	\frac{ds}{dx} \approx \frac{s_{i+1} - s_{i-1}}{2\Delta x}
\end{equation}
\begin{equation}
	\frac{d\mathbf{s}}{dx} \approx \frac{1}{2\Delta x}
	\begin{bmatrix}
		0 & 1 & 0 & \cdots & 0 & 0 \\
		-1 & 0 & 1 & \cdots & 0 & 0 \\
		0 & -1 & 0 & \cdots & 0 & 0 \\
		\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0 & 0 & \cdots & 0 & 1 \\
		0 & 0 & 0 & \cdots & -1 & 0 \\
	\end{bmatrix}
	\begin{bmatrix}
		s_1 \\
		s_2 \\
		s_3 \\
		\vdots \\
		s_{N-2} \\
		s_{N-1} \\
	\end{bmatrix}
= \frac{1}{2\Delta x} D1 \cdot \mathbf{s}
\end{equation}
代码拆解：
\begin{equation}
	\begin{aligned}
		(k(x)u_x)_x &= k_x u_x + k u_{xx}  \\
		&\approx  \frac{k_{i+1} - k_{i-1}}{2\Delta x} \cdot  \frac{u_{i+1} - u_{i-1}}{2\Delta x} + k_i \frac{u_{i-1} - 2u_i + u_{i+1}}{\Delta x^2} \\
		&=\frac{1}{4\Delta x^2} (D1\cdot \mathbf{k} \cdot D1 \cdot \mathbf{u} + 4 \cdot diag(\mathbf{k}) \cdot D2 \cdot \mathbf{u}) \\
		&=\frac{1}{4\Delta x^2} (diag(D1\cdot \mathbf{k}) \cdot D1 + 4 \cdot diag(\mathbf{k}) \cdot D2 ) \cdot \mathbf{u}
	\end{aligned}
\end{equation}
注意：这个差分格式对应着代码中的$M_{1:-1,1:-1}$，而不是M，因为代码中的M是$N \times N$，而实际做差分，边值为0，所以边界点不考虑，差分矩阵应该是$(N-2) \times (N-2)$。\\
{\color{red} (这里有疑问，为什么要一开始取$N \times N$的矩阵，后续都是使用其$(N-2) \times (N-2)$的子矩阵，为什么不一开始就构造$(N-2) \times (N-2)$？) }
\begin{equation}
	\begin{aligned}
		(v(x)u)_x &= vu_x + v_x u  \\
		&\approx  v_i \cdot  \frac{u_{i+1} - u_{i-1}}{2\Delta x} + \frac{v_{i+1} - v_{i-1}}{2\Delta x} \cdot u_i \\
		&=\frac{1}{4\Delta x^2} (2\Delta x \cdot diag(v_{1:-1}) \cdot D1_{1:-1,1:-1} + 2\Delta x \cdot diag(v_{2:} - v_{:N_x-2})\cdot \mathbf{u} \\
	\end{aligned}
\end{equation}
\begin{equation}
	1
\end{equation}
\begin{equation}
	1
\end{equation}
\begin{equation}
	M = -diag(D1 \cdot \mathbf{k})\cdot D1 -4\cdot diag(\mathbf{k})\cdot D2
\end{equation}

\begin{equation}
	m_{bond} = 8 \cdot \frac{\Delta x^2}{\Delta t} \cdot D_3 + M_{1:-1,1:-1}
\end{equation}
\begin{equation}
	v_{bond} = 2\cdot h \cdot diag(v_{1:-1}) \cdot D1_{1:-1,1:-1} + 2 \cdot h \cdot diag(v_{2:} - v_{:N_x-2})
\end{equation}
\begin{equation}
	mv_{bond} = m_{bond} + v_{bond}
\end{equation}
\begin{equation}
	c = 8 \cdot \frac{h^2}{\Delta t} \cdot D_3 - M_{1:-1,1:-1} - v_{bond}
\end{equation}
bond 是指偏微分方程的边界条件。其中 $m_{bond}$ 和 $v_{bond}$ 分别计算了扩散项和反应项的边界条件，$mv_{bond}$ 计算了边界条件的总和，c 则是经过边界条件修正后的方程系数。
\begin{equation}
	1
\end{equation}
\begin{equation}
	1
\end{equation}
\begin{equation}
	1
\end{equation}
\begin{equation}
	1
\end{equation}
\begin{equation}
	1
\end{equation}
\begin{equation}
	1
\end{equation}
\begin{equation}
	1
\end{equation}

这段代码是实现时间步进的函数，其中 u 是当前时间步的解，i 是当前时间步的编号。具体来说，它按照给定的时间步长 dt，计算下一个时间步的解。

首先，函数 g 和 dg 分别计算了 $g(u)$ 和 $g'(u)$。然后，根据离散化后的方程，计算了 $A$ 和 $b1$，其中 $A$ 是矩阵 $M_{bond}$ 和 $v_{bond}$ 的总和减去 $h^2 g'(u)$ 所组成的矩阵，$b1$ 是由 $f(x)$ 和 $g(u)$ 计算得到的向量。

接着，用 np.linalg.solve 函数求解了线性方程组 $Au=b1+b2$，其中 $b2=(c-h^2g'(u))u_i$。最后，返回更新后的解 u。

\end{document}